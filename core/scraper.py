"""
scraper.py â€” çŸ¥ä¹é¡µé¢æŠ“å– & å›¾ç‰‡ä¸‹è½½æ¨¡å—

å…è´£å£°æ˜ï¼š
æœ¬é¡¹ç›®ä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œå­¦ä¹ äº¤æµä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚
ä½¿ç”¨è€…åº”éµå®ˆçŸ¥ä¹çš„ç›¸å…³æœåŠ¡åè®®å’Œ robots.txt åè®®ã€‚
å› ä½¿ç”¨æœ¬é¡¹ç›®ä»£ç è€Œäº§ç”Ÿçš„ä»»ä½•æ³•å¾‹çº çº·æˆ–åæœï¼Œç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…ã€‚

é›†æˆ MediaCrawler çš„åçˆ¬ç­–ç•¥ï¼šPersistent Context, Stealth JS, WebGL Mock, Proxy.
"""

import asyncio
import hashlib
import json
import re
import subprocess
from pathlib import Path
from urllib.parse import urlparse
from typing import Union, List, Optional

import httpx
import execjs
from playwright.async_api import async_playwright, Playwright

from .config import get_config, get_logger

def get_auto_proxy() -> Optional[str]:
    """
    è‡ªåŠ¨è·å– macOS ç³»ç»Ÿä»£ç†è®¾ç½® (Shadowrocket/ClashX)ã€‚
    è§£æ `scutil --proxy` çš„è¾“å‡ºã€‚
    """
    try:
        output = subprocess.check_output("scutil --proxy", shell=True).decode("utf-8")
        if "HTTPEnable : 1" in output:
            # æå–ç«¯å£
            match = re.search(r"HTTPPort : (\d+)", output)
            if match:
                port = match.group(1)
                print(f"âœ… å·²è‡ªåŠ¨æ£€æµ‹åˆ°ç³»ç»Ÿä»£ç†: http://127.0.0.1:{port}")
                return f"http://127.0.0.1:{port}"
    except Exception:
        pass
    
    print("âš ï¸  æœªæ£€æµ‹åˆ°ç³»ç»Ÿä»£ç†ï¼Œå°è¯•ç›´è¿...")
    return None
# å…¨å±€é…ç½®
# è‡ªåŠ¨æ£€æµ‹æœ¬åœ°ä»£ç† (127.0.0.1:xxxx)
PROXY_SERVER = get_auto_proxy()
USER_DATA_DIR = Path(__file__).parent.parent / "browser_data"
STEALTH_JS_PATH = Path(__file__).parent.parent / "static" / "stealth.min.js"
ZHIHU_JS_PATH = Path(__file__).parent.parent / "static" / "z_core.js"


class ZhihuDownloader:
    """ä»çŸ¥ä¹æ–‡ç« /å›ç­”é¡µé¢æŠ“å– HTML å†…å®¹å¹¶ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°ã€‚"""

    _UA = (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/128.0.0.0 Safari/537.36"
    )

    _IMG_HEADERS = {
        "Referer": "https://www.zhihu.com/",
        "User-Agent": _UA,
    }

    def __init__(self, url: str) -> None:
        self.url = url.split("?")[0]
        self.page_type = self._detect_type()
        self._js_ctx = self._init_js_context()

    def _detect_type(self) -> str:
        if "zhuanlan.zhihu.com" in self.url:
            return "article"
        if "/answer/" in self.url:
            return "answer"
        if "/question/" in self.url:
            return "question"
        return "article"

    def _init_js_context(self):
        """åˆå§‹åŒ– JS æ‰§è¡Œç¯å¢ƒ (å¤‡ç”¨ï¼Œç”¨äºç”Ÿæˆ x-zse-96)ã€‚"""
        if ZHIHU_JS_PATH.exists():
            try:
                with open(ZHIHU_JS_PATH, "r", encoding="utf-8") as f:
                    js_code = f.read()
                return execjs.compile(js_code)
            except Exception as e:
                print(f"âš ï¸  JS ç¯å¢ƒåŠ è½½å¤±è´¥: {e}")
        return None

    def _get_signature(self, url: str) -> dict:
        """ç”Ÿæˆ x-zse-96 ç­¾åã€‚"""
        if not self._js_ctx:
            return {}
        try:
            # æå– path, e.g. /question/xxx
            path = urlparse(url).path
            # å‰é¢åˆ¤ç©ºäº† self._js_ctxï¼Œè¿™é‡Œæ˜¾å¼ç±»å‹æ–­è¨€æˆ–ç›´æ¥è°ƒç”¨
            from typing import cast, Any
            ctx = cast(Any, self._js_ctx)
            return ctx.call("get_sign", path, "d_c0=SEARCH_ME") # d_c0 is simplified
        except Exception as e:
            # print(f"âš ï¸  ç­¾åç”Ÿæˆå¤±è´¥: {e}")
            return {}

    # â”€â”€ é¡µé¢æŠ“å– Core â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _load_cookies(self) -> List[dict]:
        """ä» cookies.json åŠ è½½ Cookieã€‚è¿‡æ»¤æ‰å ä½ç¬¦ã€‚"""
        cookie_path = Path(__file__).parent.parent / "cookies.json"
        if cookie_path.exists():
            try:
                with open(cookie_path, "r", encoding="utf-8") as f:
                    cookies = json.load(f)
                    # è¿‡æ»¤æ‰å¸¦æœ‰å ä½ç¬¦çš„ Cookie
                    valid_cookies = [
                        c for c in cookies 
                        if c.get("value") and c.get("value") != "YOUR_COOKIE_HERE"
                    ]
                    return valid_cookies
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ cookies.json å¤±è´¥: {e}")
        return []

    def has_valid_cookies(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆ Cookie (z_c0)ã€‚"""
        try:
            cookies = self._load_cookies()
            for c in cookies:
                if c.get("name") == "z_c0" and c.get("value") and c.get("value") != "YOUR_COOKIE_HERE":
                    return True
        except:
            pass
        return False

    async def debug_dump_page(self, output_path: str = "debug_page.html"):
        """Debug purpose: dump page content to file."""
        async with async_playwright() as pw:
            browser = await pw.chromium.launch(headless=False)
            context = await browser.new_context(
                user_agent=self._UA
            )
            await context.add_init_script(path=STEALTH_JS_PATH)
            
            # Load cookies
            cookies = self._load_cookies()
            if cookies:
                await context.add_cookies(cookies)
            
            page = await context.new_page()
            try:
                await page.goto(self.url, wait_until="domcontentloaded", timeout=30000)
                await page.wait_for_timeout(5000)
                content = await page.content()
                with open(output_path, "w") as f:
                    f.write(content)
                print(f"Dumped page to {output_path}")
            except Exception as e:
                print(f"Dump failed: {e}")
            finally:
                await browser.close()

    async def fetch_page(self, **kwargs) -> Union[dict, List[dict]]:
        """
        ä½¿ç”¨ Persistent Context + Stealth + Proxy æŠ“å–é¡µé¢ã€‚
        æ”¯æŒä¼ å…¥ kwargs (å¦‚ start, limit) ä¼ é€’ç»™ _extract_questionã€‚
        """
        async with async_playwright() as pw:
            # å‡†å¤‡å¯åŠ¨å‚æ•°
            launch_args = [
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-infobars",
                "--disable-gpu",  # æœ‰æ—¶ç¦ç”¨ GPU æ›´ç¨³å®š
            ]

            # å¯åŠ¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡
            context = await pw.chromium.launch_persistent_context(
                user_data_dir=USER_DATA_DIR,
                headless=True,  # ä¾ç„¶å¯ä»¥ç”¨ Headlessï¼Œé…åˆ Stealth
                args=launch_args,
                user_agent=self._UA,
                viewport={"width": 1920, "height": 1080},
                proxy={"server": PROXY_SERVER} if PROXY_SERVER else None,
                java_script_enabled=True,
                locale="zh-CN",
                channel="chrome",  # å°è¯•ç”¨æœ¬æœº Chrome
            )

            try:
                page = context.pages[0] if context.pages else await context.new_page()

                # 1. æ³¨å…¥ stealth.min.js
                if STEALTH_JS_PATH.exists():
                    await context.add_init_script(path=STEALTH_JS_PATH)
                else:
                    print("âš ï¸  æœªæ‰¾åˆ° stealth.min.jsï¼Œåçˆ¬èƒ½åŠ›å¯èƒ½ä¸‹é™")

                # 1.5 æ³¨å…¥ Cookies
                cookies = self._load_cookies()
                if cookies:
                    await context.add_cookies(cookies)
                    print(f"ğŸª å·²åŠ è½½ {len(cookies)} ä¸ª Cookie")

                # 2. æ³¨å…¥é¢å¤–çš„ WebGL / Navigator ä¼ªé€  (å‚è€ƒ MediaCrawler CDP)
                await context.add_init_script("""
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                    
                    // ä¼ªé€  WebGL
                    const getParameter = WebGLRenderingContext.prototype.getParameter;
                    WebGLRenderingContext.prototype.getParameter = function(parameter) {
                        if (parameter === 37445) return 'Intel Inc.';
                        if (parameter === 37446) return 'Intel Iris OpenGL Engine';
                        return getParameter(parameter);
                    };
                """)

                # æ³¨å…¥ç­¾å (å¦‚æœæ˜¯ API è¯·æ±‚ï¼Œè¿™é‡Œä¸»è¦æ¼”ç¤ºæ€è·¯ï¼Œå®é™…é¡µé¢è®¿é—®ä¸éœ€è¦æ‰‹åŠ¨åŠ  Headerï¼Œæµè§ˆå™¨ä¼šè‡ªåŠ¨å¤„ç†)
                # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠç­¾ååŠ åˆ° extraHTTPHeaders
                sig = self._get_signature(self.url)
                if sig:
                   await page.set_extra_http_headers(sig)

                # 3. è®¾ç½®é»˜è®¤ Timeout
                page.set_default_timeout(30000)

                # 4. è®¿é—®é¡µé¢
                print(f"ğŸŒ è®¿é—®: {self.url}")
                # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸäºº
                await asyncio.sleep(1)
                
                await page.goto(self.url, wait_until="domcontentloaded")
                
                # ç­‰å¾… JS æ‰§è¡Œå’Œåçˆ¬æ£€æµ‹é€šè¿‡
                await page.wait_for_timeout(3000)

                # 5. å¤„ç†å¼¹çª—
                await self._dismiss_popup(page)

                # 6. æå–å†…å®¹
                # 6. æå–å†…å®¹
                if self.page_type == "article":
                    result = await self._extract_article(page)
                elif self.page_type == "question":
                    result = await self._extract_question(page, **kwargs)
                else:
                    result = await self._extract_answer(page)

                return result

            finally:
                await context.close()

    async def _dismiss_popup(self, page) -> None:
        """å…³é—­ç™»å½•å¼¹çª—ã€‚"""
        try:
            btn = page.locator("button.Modal-closeButton")
            if await btn.count() > 0:
                await btn.click(timeout=2000)
                await page.wait_for_timeout(500)
        except Exception:
            pass

    async def _extract_article(self, page) -> dict:
        """æå–æ–‡ç« ã€‚"""
        # çŸ¥ä¹çš„åçˆ¬æœ‰æ—¶ä¼šè¿”å› JSON é”™è¯¯
        text = await page.locator("body").inner_text()
        if "40362" in text or "è¯·æ±‚å­˜åœ¨å¼‚å¸¸" in text:
            raise Exception("è§¦å‘çŸ¥ä¹åçˆ¬ (40362)")

        # ç­‰å¾…æ ‡é¢˜
        await page.wait_for_selector("h1.Post-Title", timeout=10000)

        title = await page.locator("h1.Post-Title").inner_text()
        author = await self._safe_text(
            page, ".AuthorInfo span.UserLink-Name", "æœªçŸ¥ä½œè€…"
        )
        if author == "æœªçŸ¥ä½œè€…":
            author = await self._safe_text(
                page, ".AuthorInfo-name .UserLink-link", "æœªçŸ¥ä½œè€…"
            )
        
        date = await self._extract_date(page)

        # ä¼˜å…ˆç”±å®¹å™¨æ‰¾
        rich = page.locator(".Post-RichTextContainer .RichText").first
        if await rich.count() > 0:
            html = await rich.inner_html()
        else:
            html = await page.locator(".RichText").first.inner_html()

        # å°è¯•è·å–å¤´å›¾
        try:
            title_img = page.locator("img.TitleImage").first
            if await title_img.count() > 0:
                src = await title_img.get_attribute("src")
                if src:
                    html = f'<img src="{src}" alt="TitleImage"><br>{html}'
        except Exception:
            pass

        return {"title": title.strip(), "author": author.strip(), "html": html, "date": date}

    async def _extract_question(
        self, 
        page, 
        start: int = 0, 
        limit: int = 3,
        start_anchor: Optional[dict] = None,
        end_anchor: Optional[dict] = None
    ) -> List[dict]:
        """
        æå–é—®é¢˜ä¸‹çš„å¤šä¸ªå›ç­”ã€‚æ”¯æŒï¼š
        1. æ•°é‡æ¨¡å¼: ä» start å¼€å§‹æŠ“ limit ä¸ª
        2. èŒƒå›´æ¨¡å¼: ä» start_anchor (ç­”ä¸»/answer_id) æŠ“åˆ° end_anchor
        """
        text = await page.locator("body").inner_text()
        if "40362" in text or "è¯·æ±‚å­˜åœ¨å¼‚å¸¸" in text:
            raise Exception("è§¦å‘çŸ¥ä¹åçˆ¬ (40362)")

        # ç­‰å¾…é—®é¢˜æ ‡é¢˜åŠ è½½
        try:
            await page.wait_for_selector(".QuestionHeader-title", timeout=5000)
        except:
            pass
        
        # å°è¯•ç‚¹å‡» "æŸ¥çœ‹å…¨éƒ¨" æŒ‰é’® (å¦‚æœæ˜¯ auto æ¨¡å¼ä¸” limit è¾ƒå°ï¼Œå…¶å®å¯ä»¥ä¸ç‚¹ï¼Œä¸ºäº†ä¿é™©è¿˜æ˜¯ç‚¹ä¸€ä¸‹)
        await self._click_view_all(page)

        # ç­‰å¾…è‡³å°‘ä¸€ä¸ªå›ç­”é¡¹åŠ è½½
        try:
            await page.wait_for_selector(".ContentItem.AnswerItem", timeout=5000)
        except:
            print("âš ï¸ æœªæ£€æµ‹åˆ°å›ç­”åˆ—è¡¨ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–æ— å›ç­”")

        # æ™ºèƒ½æ»šåŠ¨é€»è¾‘
        if start_anchor and end_anchor:
            print(f"ğŸ¯ ç›®æ ‡: å¯»æ‰¾èŒƒå›´ {start_anchor['value']} -> {end_anchor['value']}")
            await self._scroll_until_found(page, start_anchor, end_anchor)
        else:
            target_count = start + limit
            print(f"ğŸ¯ ç›®æ ‡: æŠ“å–å‰ {target_count} ä¸ªå›ç­”")
            await self._scroll_until_count(page, target_count)
        
        # è·å–æ‰€æœ‰å›ç­”å¡ç‰‡
        answers = page.locator(".ContentItem.AnswerItem")
        total_found = await answers.count()
        
        # è®¡ç®—æå–èŒƒå›´
        extract_indices = []
        
        if start_anchor and end_anchor:
            # èŒƒå›´æ¨¡å¼
            print(f"ğŸ“Š æ­£åœ¨å®šä½èµ·æ­¢ç‚¹...")
            start_idx, end_idx = -1, -1
            
            # éå†æ‰€æœ‰å›ç­”å»ºç«‹ç´¢å¼•
            for i in range(total_found):
                item = answers.nth(i)
                info = await self._get_card_info(item)
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é… Start
                if start_idx == -1:
                    if self._match_anchor(info, start_anchor):
                        start_idx = i
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é… End (End å¿…é¡» >= Start)
                if end_idx == -1:
                    if self._match_anchor(info, end_anchor):
                        end_idx = i
            
            if start_idx != -1 and end_idx != -1:
                # ç¡®ä¿é¡ºåºæ­£ç¡®
                if start_idx > end_idx:
                    print(f"âš ï¸ èµ·å§‹ä½ç½®({start_idx})åœ¨ç»“æŸä½ç½®({end_idx})ä¹‹åï¼Œè‡ªåŠ¨äº¤æ¢...")
                    start_idx, end_idx = end_idx, start_idx
                
                print(f"âœ… é”å®šèŒƒå›´: ç´¢å¼• [{start_idx}] -> [{end_idx}] (å…± {end_idx - start_idx + 1} ä¸ª)")
                extract_indices = list(range(start_idx, end_idx + 1))
            else:
                 print(f"âŒ æœªèƒ½å®Œå…¨æ‰¾åˆ°èµ·æ­¢ç‚¹ (Start found: {start_idx}, End found: {end_idx})")
                 print("   å°†å°è¯•æå–æ‰€æœ‰å·²åŠ è½½å†…å®¹...")
                 extract_indices = list(range(total_found))
        else:
            # æ•°é‡æ¨¡å¼
            target_count = start + limit
            actual_limit = min(total_found, target_count)
            print(f"ğŸ“Š å‡†å¤‡æå–èŒƒå›´ [{start}:{actual_limit}]...")
            extract_indices = list(range(start, actual_limit))

        results = []
        question_title = await self._safe_text(page, "h1.QuestionHeader-title", "æœªçŸ¥é—®é¢˜")

        for i in extract_indices:

            item = answers.nth(i)
            try:
                data = await self._parse_answer_element(item, page, question_title)
                results.append(data)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ç¬¬ {i+1} ä¸ªå›ç­”: {e}")
        
        return results

    async def _scroll_until_count(self, page, target_count: int):
        """æ»šåŠ¨ç›´åˆ°è¾¾åˆ°ç›®æ ‡æ•°é‡ã€‚"""
        prev_count = 0
        no_change_count = 0
        max_attempts = 50

        while True:
            count = await page.locator(".ContentItem.AnswerItem").count()
            print(f"ğŸ”„ å½“å‰åŠ è½½äº† {count} ä¸ªå›ç­” (ç›®æ ‡: {target_count})...")
            
            if count >= target_count:
                break
            
            if count == prev_count:
                no_change_count += 1
                
                # å°è¯•å†æ¬¡ç‚¹å‡» "æŸ¥çœ‹å…¨éƒ¨"ï¼Œé˜²æ¼
                if no_change_count % 2 == 0:
                     await self._click_view_all(page)

                # å¦‚æœå¡ä½å¤ªä¹…ï¼Œå°è¯•åˆ‡æ¢æ’åº
                if no_change_count == 4:
                    print("ğŸ”„ å°è¯•åˆ‡æ¢æ’åºæ–¹å¼ (æŒ‰æ—¶é—´æ’åº)...")
                    await self._switch_sort_order(page)
                    no_change_count = 0 # é‡ç½®è®¡æ•°ï¼Œç»™æ–°æ’åºä¸€ç‚¹æœºä¼š
                    continue

                if no_change_count >= 8: # å¢åŠ å°è¯•æ¬¡æ•°
                    print("âš ï¸  å·²æ»šåŠ¨åˆ°åº•éƒ¨æˆ–æ— æ³•åŠ è½½æ›´å¤š")
                    # Debug: Dump HTML & Buttons
                    try:
                        with open("debug_failed_scroll.html", "w", encoding="utf-8") as f:
                            f.write(await page.content())
                        print("ğŸ’¾ å·²ä¿å­˜è°ƒè¯•é¡µé¢: debug_failed_scroll.html")
                        
                        btns = page.locator("button")
                        cnt = await btns.count()
                        print(f"ğŸ” é¡µé¢å‰©ä½™æŒ‰é’® ({cnt}ä¸ª):")
                        for i in range(min(cnt, 20)):
                            txt = await btns.nth(i).inner_text()
                            if txt.strip():
                                clean_txt = txt.strip().replace('\n', ' ')
                                print(f"   [Btn] {clean_txt}")
                    except: pass
                    break
            else:
                no_change_count = 0
            
            prev_count = count
            await self._scroll_step(page)
            
            max_attempts -= 1
            if max_attempts <= 0:
                break

    async def _scroll_until_found(self, page, start_anchor, end_anchor):
        """æ»šåŠ¨ç›´åˆ°æ‰¾åˆ°èµ·æ­¢é”šç‚¹ï¼ˆæˆ–è¾¾åˆ°ä¸Šé™ï¼‰ã€‚"""
        limit = 200 # é˜²æ­¢æ— é™æ»šåŠ¨
        prev_count = 0
        no_change_count = 0
        
        while True:
            answers = page.locator(".ContentItem.AnswerItem")
            count = await answers.count()
            print(f"ğŸ”„ æ­£åœ¨æœç´¢é”šç‚¹... (å½“å‰ {count} ä¸ª)")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å« start å’Œ end
            found_start = False
            found_end = False
            
            # è¿™é‡Œçš„æ£€æŸ¥æ¯”è¾ƒè€—æ—¶ï¼Œæ¯ 5 æ¬¡æˆ–è€…æ»šåŠ¨åœæ»æ—¶æ£€æŸ¥ä¸€æ¬¡æ¯”è¾ƒå¥½
            # ä¸ºäº†å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬ç®€å•ç²—æš´ç‚¹ï¼Œæ¯æ¬¡éƒ½æ£€æŸ¥æœ€åå‡ ä¸ª? 
            # è¿˜æ˜¯ç›´æ¥æ£€æŸ¥å…¨éƒ¨? æ£€æŸ¥å…¨éƒ¨æ¯”è¾ƒç¨³å¦¥
            
            # ä¼˜åŒ–: åªåœ¨æ•°é‡å˜åŒ–æˆ–è€…æ¯éš”å‡ æ¬¡æ£€æŸ¥
            # è¿™é‡Œç®€åŒ–é€»è¾‘: æ¯æ¬¡æ£€æŸ¥æœ€å 5 ä¸ªçœ‹æ˜¯å¦åŒ…å« end? 
            # ä¸è¡Œï¼Œend å¯èƒ½æ—©å°±åŠ è½½è¿‡äº†ï¼Œæˆ–è€… start å’Œ end å¾ˆè¿‘
            
            # ç®€å•ç­–ç•¥: åªè¦æ²¡æœ‰åŒæ—¶æ‰¾åˆ°ä¸¤ä¸ªï¼Œå°±ä¸€ç›´æ»šï¼Œç›´åˆ°ä¸Šé™
            # ä½†æˆ‘ä»¬éœ€è¦çŸ¥é“æ˜¯å¦å·²ç»æ‰¾åˆ°äº†
            
            # æˆ‘ä»¬å¯ä»¥æŠ½æ ·æ£€æŸ¥:
            # å€’åºæ£€æŸ¥
            # for i in range(count - 1, -1, -1):
            
            # å®é™…ä¸Šï¼Œåªè¦ count æ²¡å˜ï¼Œå°±æ„å‘³ç€åˆ°åº•äº†
            if count >= limit:
                print(f"âš ï¸ è¾¾åˆ°æ»šåŠ¨ä¸Šé™ ({limit})")
                break

            if count == prev_count:
                no_change_count += 1
                
                # å°è¯•å†æ¬¡ç‚¹å‡» "æŸ¥çœ‹å…¨éƒ¨"
                if no_change_count % 2 == 0:
                     await self._click_view_all(page)

                # å°è¯•åˆ‡æ¢æ’åº
                if no_change_count == 4:
                    # é™ä½æ—¥å¿—çº§åˆ«æˆ–ä¿®æ”¹ä¸º rich print (å¦‚æœå¼•å…¥äº†)
                    # print("ğŸ”„ å°è¯•åˆ‡æ¢æ’åºæ–¹å¼ (æŒ‰æ—¶é—´æ’åº)...") 
                    await self._switch_sort_order(page)
                    no_change_count = 0 
                    continue

                if no_change_count >= 8:
                    # print("âš ï¸  å·²æ»šåŠ¨åˆ°åº•éƒ¨")
                    break
            else:
                no_change_count = 0
            
            # æ£€æµ‹é€»è¾‘ï¼šå¦‚æœ count æ¯”è¾ƒå¤§äº†ï¼Œæˆ‘ä»¬å¯ä»¥è¯•ç€æ‰¾ä¸€ä¸‹
            # ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬æ¯å¢åŠ  10 ä¸ªæˆ–è€…æ»šåŠ¨ 5 æ¬¡æ£€æµ‹ä¸€æ¬¡ï¼Ÿ
            # æš‚æ—¶å…ˆç”¨æœ€ç®€å•çš„ï¼šä¸€ç›´æ»šåˆ°åº•éƒ¨æˆ–è€…ä¸Šé™ï¼Œæœ€åå†åŒ¹é…ã€‚
            # ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºä¸­é—´æ£€æµ‹ DOM å¾ˆæ…¢ã€‚
            # ç”¨æˆ·ä½“éªŒä¼˜åŒ–ï¼šå¦‚æœç”¨æˆ·çŸ¥é“ end åœ¨å‰ 50 ä¸ªï¼Œæ»šåˆ° 200 ä¸ªå¤ªæ…¢ã€‚
            
            # æŠ˜ä¸­æ–¹æ¡ˆï¼šå…ˆä¸åšå®æ—¶æ£€æµ‹ï¼Œä¾èµ– limit å’Œæ‰‹åŠ¨åœæ­¢ã€‚
            # æˆ–è€…ï¼šæ¯æ¬¡æ»šåŠ¨åï¼Œåªæ£€æŸ¥æ–°åŠ è½½çš„ items? 
            # ç®—äº†ï¼Œä¿æŒç®€å•ï¼Œç›´æ¥å¤ç”¨æ»šåŠ¨é€»è¾‘ï¼ŒæŠŠ limit è®¾å¤§ä¸€ç‚¹ã€‚
            # ä½†æ˜¯ä¸ºäº†"æ‰¾åˆ°å³åœ"ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥ã€‚
            
            # è®©æˆ‘ä»¬å°è¯•å¿«é€Ÿæ£€æŸ¥ä¸€ä¸‹é¡µé¢æ–‡æœ¬?
            # page_text = await page.inner_text() 
            # if start_anchor['value'] in page_text and end_anchor['value'] in page_text:
            #    break
            # è¿™ä¹Ÿå¾ˆæ…¢ã€‚
            
            # é‡‡ç”¨æ–¹æ¡ˆ: æ»š 5 æ¬¡æ£€æŸ¥ä¸€æ¬¡ metadata
            pass 

            prev_count = count
            await self._scroll_step(page)

    async def _scroll_step(self, page):
        """æ‰§è¡Œä¸€æ¬¡æ»šåŠ¨åŠ¨ä½œã€‚"""
        # ä½¿ç”¨ JS æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œé€šå¸¸æ¯”å•çº¯é¼ æ ‡æ»šè½®æ›´æœ‰æ•ˆè§¦å‘åŠ è½½
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(1)
        # é…åˆ End é”®
        await page.keyboard.press("End")
        await asyncio.sleep(1)

    async def _get_card_info(self, item) -> dict:
        """è·å–å›ç­”å¡ç‰‡çš„å…ƒæ•°æ®ç”¨äºåŒ¹é…ã€‚"""
        # æå– answer_id
        answer_id = ""
        try:
             # data-zop="{... "itemId":12345 ...}"
             zop = await item.get_attribute("data-zop")
             if zop:
                 if '"itemId":' in zop:
                     import json
                     # ç®€å•çš„å­—ç¬¦ä¸²æå–ï¼Œæ¯” json.loads å¿«ä¸”å®¹é”™
                     m = re.search(r'"itemId":(\d+)', zop)
                     if m: answer_id = m.group(1)
        except: pass
        if not answer_id:
             # try name attribute
             answer_id = await item.get_attribute("name") or ""
        
        # æå– author
        author = await self._safe_text(item, ".AuthorInfo-name .UserLink-link", "")
        if not author:
             author = await self._safe_text(item, ".AuthorInfo span.UserLink-Name", "")
             
        return {"answer_id": str(answer_id), "author": author.strip()}

    def _match_anchor(self, info: dict, anchor: dict) -> bool:
        """åˆ¤æ–­å¡ç‰‡æ˜¯å¦åŒ¹é…é”šç‚¹ã€‚"""
        if not anchor: return False
        
        val = str(anchor["value"]).strip()
        
        if anchor["type"] == "answer_id":
            return val == info.get("answer_id")
        
        if anchor["type"] == "author":
             # æ¨¡ç³ŠåŒ¹é…? è¿˜æ˜¯ç²¾ç¡®? ç²¾ç¡®æ¯”è¾ƒå¥½ï¼Œé˜²æ­¢åŒåè¯¯ä¼¤
             # çŸ¥ä¹ id ä¸€èˆ¬æ˜¯å”¯ä¸€çš„ï¼Œä½†åå­—ä¸ä¸€å®šã€‚
             return val == info.get("author")
             
        return False

    async def _click_view_all(self, page):
        """ç‚¹å‡» 'æŸ¥çœ‹å…¨éƒ¨' æŒ‰é’®çš„å°è£…ã€‚"""
        candidates = [
            "button.QuestionMainAction-ViewAll",
            "a.QuestionMainAction-ViewAll",
            "div.Question-mainColumn button:has-text('æŸ¥çœ‹å…¨éƒ¨')",
            "div.Question-mainColumn button:has-text('æ›´å¤šå›ç­”')",
            "div.Question-mainColumn button:has-text('å±•å¼€é˜…è¯»å…¨æ–‡')",
            "div.Question-mainColumn button:has-text('æ˜¾ç¤ºå…¨éƒ¨')",
             # å…œåº•ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«ç‰¹å®šæ–‡æœ¬çš„æŒ‰é’®
            "button:has-text('View All')",
            "button:has-text('More Answers')",
            "button:has-text('æ˜¾ç¤ºå…¨éƒ¨')"
        ]
        
        for sel in candidates:
            try:
                # ä½¿ç”¨ first é¿å…å¤šåŒ¹é…æŠ¥é”™
                btn = page.locator(sel).first
                if await btn.count() > 0 and await btn.is_visible():
                    print(f"ğŸ‘† å°è¯•ç‚¹å‡»: {sel}")
                    await btn.click()
                    # ç­‰å¾…å†…å®¹åŠ è½½
                    await asyncio.sleep(2)
                    return True
            except:
                pass
        return False

    async def _switch_sort_order(self, page):
        """åˆ‡æ¢æ’åºæ–¹å¼ï¼ˆé»˜è®¤ -> æŒ‰æ—¶é—´ï¼‰ï¼Œæœ‰æ—¶èƒ½è§£å†³åŠ è½½å¡é¡¿é—®é¢˜ã€‚"""
        try:
            # 1. æ‰¾åˆ°æ’åºæŒ‰é’® (é€šå¸¸æ˜¯ 'é»˜è®¤æ’åº')
            sort_btn = page.locator("button:has-text('é»˜è®¤æ’åº')").first
            if await sort_btn.count() == 0:
                print("âš ï¸ æœªæ‰¾åˆ° 'é»˜è®¤æ’åº' æŒ‰é’®ï¼Œè·³è¿‡åˆ‡æ¢")
                return

            print("ğŸ‘† ç‚¹å‡» 'é»˜è®¤æ’åº'...")
            await sort_btn.click()
            await asyncio.sleep(1)

            # 2. ç‚¹å‡» 'æŒ‰æ—¶é—´æ’åº'
            time_sort = page.locator("button:has-text('æŒ‰æ—¶é—´æ’åº')").first
            if await time_sort.count() > 0:
                print("ğŸ‘† åˆ‡æ¢åˆ° 'æŒ‰æ—¶é—´æ’åº'...")
                await time_sort.click()
                await asyncio.sleep(3) # ç­‰å¾…åˆ·æ–°
            else:
                 print("âš ï¸ æœªæ‰¾åˆ° 'æŒ‰æ—¶é—´æ’åº' é€‰é¡¹")
                 # å…³é—­èœå• (ç‚¹åˆ«å¤„)
                 await page.mouse.click(0, 0)
        except Exception as e:
             print(f"âš ï¸ åˆ‡æ¢æ’åºå¤±è´¥: {e}")

    async def _extract_answer(self, page) -> dict:
        """æå–å•ä¸ªå›ç­”ã€‚"""
        text = await page.locator("body").inner_text()
        if "40362" in text:
            raise Exception("è§¦å‘çŸ¥ä¹åçˆ¬ (40362)")

        # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œæ”¹ç”¨æ›´å®½æ³›çš„é€‰æ‹©å™¨ï¼Œé¿å… strictly waiting for .QuestionAnswer-content
        try:
            # ä¼˜å…ˆç­‰å¾…å›ç­”ä¸»ä½“ï¼Œç»™ 15s è¶…æ—¶
            await page.wait_for_selector(".ContentItem.AnswerItem", timeout=15000)
        except:
            print("âš ï¸  ç­‰å¾…å›ç­”å†…å®¹è¶…æ—¶ï¼Œå°è¯•ç›´æ¥è§£æ...")
        
        # å°è¯•ä» URL æå– answer_id
        answer_id = None
        match = re.search(r"answer/(\d+)", self.url)
        if match:
            answer_id = match.group(1)
            
        # ç¡®å®šå†…å®¹å®¹å™¨
        container = page.locator(".ContentItem.AnswerItem").first
        
        if answer_id:
            # å°è¯•ç²¾ç¡®å®šä½
            specific_item = page.locator(f".ContentItem.AnswerItem[name='{answer_id}']")
            if await specific_item.count() > 0:
                print(f"ğŸ¯ å®šä½åˆ°æŒ‡å®šå›ç­”: {answer_id}")
                container = specific_item.first
            else:
                zop_item = page.locator(f".ContentItem.AnswerItem[data-zop*='{answer_id}']")
                if await zop_item.count() > 0:
                    print(f"ğŸ¯ é€šè¿‡ data-zop å®šä½åˆ°æŒ‡å®šå›ç­”: {answer_id}")
                    container = zop_item.first
        
        # è·å–é—®é¢˜æ ‡é¢˜
        question_title = await self._safe_text(page, "h1.QuestionHeader-title", "æœªçŸ¥é—®é¢˜")
        
        return await self._parse_answer_element(container, page, question_title)

    async def _parse_answer_element(self, element, page, question_title) -> dict:
        """è§£æå•ä¸ªå›ç­”å…ƒç´ """
        # ä½œè€…
        author = await self._safe_text(element, ".AuthorInfo-name .UserLink-link", "æœªçŸ¥ä½œè€…")
        if author == "æœªçŸ¥ä½œè€…":
            author = await self._safe_text(element, ".AuthorInfo span.UserLink-Name", "æœªçŸ¥ä½œè€…")
        
        # èµåŒæ•°
        upvotes_text = await self._safe_text(element, "button.VoteButton--up", "0")
        # æå–æ•°å­—, e.g. "èµåŒ 1.2 ä¸‡" -> 12000
        upvotes = self._parse_upvotes(upvotes_text)

        # å‘å¸ƒæ—¶é—´
        date = await self._extract_date(element)
        
        # å†…å®¹ HTML
        rich = element.locator(".RichText").first
        if await rich.count() > 0:
            html = await rich.inner_html()
        else:
             html = "<p>æ— æ³•è·å–å†…å®¹</p>"

        return {
            "title": question_title.strip(), 
            "author": author.strip(), 
            "html": html, 
            "date": date,
            "upvotes": upvotes
        }

    def _parse_upvotes(self, text: str) -> int:
        """è§£æèµåŒæ•°æ–‡æœ¬ã€‚"""
        # e.g. "èµåŒ 1,234", "1.2 ä¸‡", "750"
        m = re.search(r"([\d\.,]+)\s*([ä¸‡kK]?)", text)
        if not m: return 0
        num_str = m.group(1).replace(",", "")
        unit = m.group(2).lower()
        try:
            val = float(num_str)
            if unit == "ä¸‡": val *= 10000
            elif unit in ("k", "K"): val *= 1000
            return int(val)
        except:
            return 0

    async def _extract_date(self, element) -> str:
        from datetime import date as dt_date
        try:
            # 1. å°è¯•æ‰¾ meta (é€‚ç”¨äº Page æˆ–åŒ…å« meta çš„å®¹å™¨)
            meta = await element.locator('meta[itemprop="datePublished"]').get_attribute("content", timeout=500)
            if meta: return meta[:10]
        except: pass
        
        try:
            # 2. å°è¯•æ‰¾ "å‘å¸ƒäº ..." æ–‡æœ¬ (é€‚ç”¨äº AnswerItem)
            text = await element.locator(".ContentItem-time").first.inner_text(timeout=500)
            m = re.search(r"(\d{4}-\d{2}-\d{2})", text)
            if m: return m.group(1)
        except: pass

        return dt_date.today().isoformat()

    async def _safe_text(self, page, selector: str, default: str) -> str:
        try:
            el = page.locator(selector).first
            return await el.inner_text(timeout=2000)
        except:
            return default

    # â”€â”€ å›¾ç‰‡ä¸‹è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @classmethod
    async def download_images(
        cls,
        img_urls: List[str],
        dest: Path,
        *,
        concurrency: int = 4,
        timeout: float = 30.0,
    ) -> dict[str, str]:
        """
        å¹¶å‘ä¸‹è½½å›¾ç‰‡

        Args:
            img_urls: å›¾ç‰‡ URL åˆ—è¡¨
            dest: ä¿å­˜ç›®å½•
            concurrency: å¹¶å‘æ•° (é»˜è®¤ 4)
            timeout: è¶…æ—¶æ—¶é—´ (ç§’)
        """
        if not img_urls:
            return {}

        dest.mkdir(parents=True, exist_ok=True)
        url_to_local: dict[str, str] = {}

        # ä»é…ç½®è¯»å–å¹¶å‘æ•°
        try:
            cfg = get_config()
            concurrency = cfg.crawler.images.concurrency
            timeout = cfg.crawler.images.timeout
        except Exception:
            pass  # ä½¿ç”¨é»˜è®¤å€¼

        limits = httpx.Limits(
            max_keepalive_connections=concurrency,
            max_connections=concurrency * 2,
        )

        async with httpx.AsyncClient(
            headers=cls._IMG_HEADERS,
            timeout=timeout,
            follow_redirects=True,
            proxy=PROXY_SERVER,
            limits=limits,
        ) as client:
            # ä½¿ç”¨ Semaphore é™åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(concurrency)

            async def download_with_limit(url: str) -> None:
                async with semaphore:
                    await cls._download_one(client, url, dest, url_to_local)

            tasks = [download_with_limit(url) for url in img_urls]
            await asyncio.gather(*tasks, return_exceptions=True)

        # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥æ•°
        success = sum(1 for v in url_to_local.values() if v)
        log = get_logger()
        log.info(
            "images_downloaded",
            total=len(img_urls),
            success=success,
            failed=len(img_urls) - success,
        )

        return url_to_local

    @staticmethod
    async def _download_one(client, url, dest, mapping):
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            if url.startswith("//"):
                url = "https:" + url

            # è¿‡æ»¤ä¸éœ€è¦çš„å›¾ç‰‡
            if "data:image" in url or "equation" in url:
                return

            resp = await client.get(url)
            resp.raise_for_status()

            ext = Path(urlparse(url).path).suffix or ".jpg"
            if len(ext) > 5:
                ext = ".jpg"

            fname = hashlib.md5(url.encode()).hexdigest()[:12] + ext
            fpath = dest / fname

            fpath.write_bytes(resp.content)
            # å¿…é¡»ç”¨ / åˆ†éš”ï¼Œè¦åœ¨ Markdown é‡Œç”¨
            mapping[url] = f"images/{fname}"
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œè®°å½•åˆ°æ—¥å¿—
            logger = get_logger()
            logger.warning("image_download_failed", url=url[:50], error=str(e)[:50])