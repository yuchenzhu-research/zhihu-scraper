"""
scraper.py â€” çŸ¥ä¹é¡µé¢æŠ“å– & å›¾ç‰‡ä¸‹è½½æ¨¡å— (v3.0 çº¯åè®®å¼•æ“ API ç‰ˆ)

å…è´£å£°æ˜ï¼š
æœ¬é¡¹ç›®ä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œå­¦ä¹ äº¤æµä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚
ä½¿ç”¨è€…åº”éµå®ˆçŸ¥ä¹çš„ç›¸å…³æœåŠ¡åè®®å’Œ robots.txt åè®®ã€‚

é›†æˆçº¯åè®®å±‚ç½‘ç»œå®¢æˆ·ç«¯ï¼Œç›´æ¥åŸºäº v4 API æŠ“å–ã€‚
å¤§å¹…åº¦é™ä½å¯¹ CPU å’Œå†…å­˜çš„æ¶ˆè€—ï¼Œé˜²å°æ ¸å¿ƒä¾èµ–äº TLS æŒ‡çº¹æ¨¡æ‹Ÿ (curl_cffi)ã€‚
"""

import asyncio
from typing import Union, List, Optional
import httpx
from pathlib import Path
import re
from datetime import datetime

from .config import get_logger, get_humanizer
from .api_client import ZhihuAPIClient

class ZhihuDownloader:
    """ä»çŸ¥ä¹æ–‡ç« /å›ç­”é¡µé¢ç›´æ¥æŠ“å– API æ•°æ®å¹¶ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°ã€‚"""

    def __init__(self, url: str) -> None:
        self.url = url.split("?")[0]
        self.page_type = self._detect_type()
        self.api_client = ZhihuAPIClient()
        self.log = get_logger()

    def _detect_type(self) -> str:
        if "zhuanlan.zhihu.com" in self.url:
            return "article"
        if "/answer/" in self.url:
            return "answer"
        if "/question/" in self.url:
            return "question"
        return "article"

    def has_valid_cookies(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆ Cookie (å…¼å®¹ CLI è°ƒç”¨)ã€‚"""
        return bool(self.api_client._cookies_dict)

    async def fetch_page(self, **kwargs) -> Union[dict, List[dict]]:
        """
        ä½¿ç”¨çº¯åè®®å±‚æŠ“å–é¡µé¢æ•°æ®ã€‚
        æ”¯æŒä¼ å…¥ kwargs (å¦‚ start, limit) ä¼ é€’ç»™ _extract_questionã€‚
        æ³¨æ„ï¼šç›®å‰å·²ç»æ˜¯è½»é‡çº§ä»£ç ï¼Œä½†ä¸ºäº†å…¼å®¹ v2.0 çš„åç¨‹å¤–å£³ï¼Œæ–¹æ³•ç­¾åä¿ç•™ asyncã€‚
        """
        humanizer = get_humanizer()
        
        self.log.info("start_fetching", url=self.url, page_type=self.page_type)
        print(f"ğŸŒ è®¿é—® [API æ¨¡å¼]: {self.url}")
        
        # æ¨¡æ‹Ÿéƒ¨åˆ†å»¶æ—¶ï¼Œä»¥é¿å…ç¬æ—¶é«˜é¢‘è¯·æ±‚
        await humanizer.page_load()

        if self.page_type == "article":
            return self._extract_article()
        elif self.page_type == "question":
            return self._extract_question(**kwargs)
        else:
            return self._extract_answer()

    def _extract_article(self) -> dict:
        """æå–ä¸“æ æ–‡ç« æ•°æ®ã€‚"""
        # ä» URL æå– Article ID
        # e.g., https://zhuanlan.zhihu.com/p/123456
        match = re.search(r"p/(\d+)", self.url)
        if not match:
             raise Exception(f"æ— æ³•ä»ä¸“æ  URL æå– ID: {self.url}")
        
        article_id = match.group(1)
        data = self.api_client.get_article(article_id)
        
        author = data.get("author", {}).get("name", "æœªçŸ¥ä½œè€…")
        title = data.get("title", "æœªçŸ¥ä¸“æ æ ‡é¢˜")
        html = data.get("content", "")
        upvotes = data.get("voteup_count", 0)
        
        # å°† timestamp è½¬ä¸ºæ—¥å†æ ¼å¼
        created_sec = data.get("created", 0)
        date_str = datetime.fromtimestamp(created_sec).strftime("%Y-%m-%d") if created_sec else datetime.today().strftime("%Y-%m-%d")

        # æŒ‚è½½å¤´å›¾
        title_img = data.get("image_url")
        if title_img:
            html = f'<img src="{title_img}" alt="TitleImage"><br>{html}'

        return {
            "id": article_id,
            "type": "article",
            "url": self.url,
            "title": title.strip(), 
            "author": author.strip(), 
            "html": html, 
            "date": date_str,
            "upvotes": upvotes
        }

    def _extract_answer(self) -> dict:
        """æå–å•ä¸ªå›ç­”æ•°æ®ã€‚"""
        # https://www.zhihu.com/question/298203515/answer/2008258573281562692
        match = re.search(r"answer/(\d+)", self.url)
        if not match:
             raise Exception(f"æ— æ³•ä»å›ç­” URL æå– ID: {self.url}")
             
        answer_id = match.group(1)
        data = self.api_client.get_answer(answer_id)
        
        author = data.get("author", {}).get("name", "æœªçŸ¥ä½œè€…")
        title = data.get("question", {}).get("title", "æœªçŸ¥é—®é¢˜")
        html = data.get("content", "")
        upvotes = data.get("voteup_count", 0)
        
        created_sec = data.get("created_time", 0)
        date_str = datetime.fromtimestamp(created_sec).strftime("%Y-%m-%d") if created_sec else datetime.today().strftime("%Y-%m-%d")

        return {
            "id": answer_id,
            "type": "answer",
            "url": self.url,
            "title": title.strip(), 
            "author": author.strip(), 
            "html": html, 
            "date": date_str,
            "upvotes": upvotes
        }

    def _extract_question(self, start: int = 0, limit: int = 3, **kwargs) -> List[dict]:
        """æå–é—®é¢˜ä¸‹çš„å¤šä¸ªå›ç­”ã€‚åˆ©ç”¨ API åˆ†é¡µç›´æ¥è·å–ï¼Œæ— è§† DOM æ»šåŠ¨ã€‚"""
        match = re.search(r"question/(\d+)", self.url)
        if not match:
             raise Exception(f"æ— æ³•ä»é—®é¢˜ URL æå– ID: {self.url}")
             
        question_id = match.group(1)
        
        # ä¸ºäº†é˜²å°ï¼Œå¯ä»¥ä¸€æ¬¡æ‹¿ä¸€é¡µï¼ˆå¦‚ limit=20 å†…ï¼‰ï¼Œå¦‚æœä½ éœ€è¦å¾ˆå¤šï¼Œéœ€è¦åœ¨è¿™é‡Œå¾ªç¯
        # å¦‚æœ limit å¾ˆå¤§ï¼Œå»ºè®®ä½¿ç”¨ for å¾ªç¯å¸¦ delay åˆ†é¡µæ‹¿
        print(f"ğŸ¯ ç›®æ ‡: API æŠ“å–é—®é¢˜ {question_id} çš„å‰ {limit} ä¸ªå›ç­” (ä»ç¬¬ {start} åªå¼€å§‹)")
        
        answers_data = self.api_client.get_question_answers(question_id, limit=limit, offset=start)
        
        results = []
        for data in answers_data:
            author = data.get("author", {}).get("name", "æœªçŸ¥ä½œè€…")
            title = data.get("question", {}).get("title", "æœªçŸ¥é—®é¢˜")
            html = data.get("content", "")
            upvotes = data.get("voteup_count", 0)
            
            created_sec = data.get("created_time", 0)
            date_str = datetime.fromtimestamp(created_sec).strftime("%Y-%m-%d") if created_sec else datetime.today().strftime("%Y-%m-%d")

            results.append({
                "id": str(data.get("id", "")),
                "type": "answer",
                "url": f"https://www.zhihu.com/question/{question_id}/answer/{data.get('id', '')}",
                "title": title.strip(), 
                "author": author.strip(), 
                "html": html, 
                "date": date_str,
                "upvotes": upvotes
            })
            
        print(f"âœ… æˆåŠŸå‘½ä¸­ {len(results)} ä¸ªå›ç­”ã€‚")
        return results

    # â”€â”€ å›¾ç‰‡ä¸‹è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @classmethod
    async def download_images(
        cls,
        img_urls: List[str],
        dest: Path,
        *,
        concurrency: int = 4,
        timeout: float = 30.0,
    ) -> dict[str, str]:
        """
        å¹¶å‘ä¸‹è½½å›¾ç‰‡ (ä¿æŒä½¿ç”¨è½»é‡çš„ httpx å®¢æˆ·ç«¯è¿›è¡ŒåŸºç¡€èµ„æºä¸‹è½½)
        """
        if not img_urls:
            return {}

        dest.mkdir(parents=True, exist_ok=True)
        url_to_local: dict[str, str] = {}
        sem = asyncio.Semaphore(concurrency)
        client = httpx.AsyncClient(headers={
            "Referer": "https://www.zhihu.com/",
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
        })

        async def worker(url: str):
            async with sem:
                try:
                    # è·å–æ–‡ä»¶å
                    file_name = url.split("/")[-1]
                    if "?" in file_name:
                        file_name = file_name.split("?")[0]
                    # è¡¥å…¨æ‰©å±•å
                    if not "." in file_name:
                         file_name += ".jpg"
                    
                    local_path = dest / file_name
                    
                    if local_path.exists():
                        url_to_local[url] = file_name
                        return
                        
                    resp = await client.get(url, timeout=timeout)
                    resp.raise_for_status()
                    with open(local_path, "wb") as f:
                        f.write(resp.content)
                        
                    url_to_local[url] = file_name
                    
                except Exception as e:
                    # ä½¿ç”¨ print ä»£æ›¿ log é¿å…é˜»å¡è¿‡æ·±
                    print(f"âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ [{url}]: {e}")

        # å¹¶å‘æ‰§è¡Œ
        tasks = [worker(url) for url in img_urls]
        await asyncio.gather(*tasks)
        await client.aclose()

        return url_to_local