"""
cli/app.py â€” CLI å¢å¼ºæ¨¡å—

ä½¿ç”¨ Typer æä¾›ç°ä»£åŒ–å‘½ä»¤è¡Œæ¥å£ï¼Œæ”¯æŒå‚æ•°è‡ªåŠ¨è¡¥å…¨ã€‚
"""

from pathlib import Path
from typing import Optional, List, Dict, Any
from random import uniform
import asyncio
import typer
from rich import print as rprint
from rich.panel import Panel
from rich.text import Text

from core.config import get_config, get_logger, get_humanizer
from core.scraper import ZhihuDownloader
from core.converter import ZhihuConverter
from core.errors import handle_error

# ============================================================
# CLI åº”ç”¨åˆå§‹åŒ–
# ============================================================

app = typer.Typer(
    name="zhihu",
    help="ğŸ•·ï¸ çŸ¥ä¹é«˜è´¨é‡å†…å®¹çˆ¬å–å·¥å…·",
    add_completion=False,
    no_args_is_help=True,
)

# åˆå§‹åŒ–é…ç½®å’Œæ—¥å¿—
cfg = get_config()
log = get_logger()


# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================

def sanitize_filename(name: str) -> str:
    """æ¸…ç†æ–‡ä»¶åéæ³•å­—ç¬¦"""
    import re
    name = re.sub(r'[/\\:*?"<>|\x00-\x1f]', "_", name)
    name = name.strip(" .")
    return name[:50] or "untitled"


def extract_urls(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–çŸ¥ä¹é“¾æ¥"""
    import re
    pattern = r"(?:https?://)?(?:www\.|zhuanlan\.)?zhihu\.com/(?:p/\d+|question/\d+(?:/answer/\d+)?)"
    matches = re.findall(pattern, text)
    return list(dict.fromkeys([(m if m.startswith("http") else "https://" + m) for m in matches]))


def print_result(
    title: str,
    author: str,
    success: bool,
    path: str = "",
    error: Optional[str] = None
) -> None:
    """æ‰“å°å•æ¡ç»“æœ"""
    if success:
        rprint(f"âœ… [bold cyan]{author}[/] - [white]{title[:30]}...[/]")
        rprint(f"   ğŸ“ {path}")
    else:
        rprint(f"âŒ [bold cyan]{author}[/] - [yellow]{title[:30]}...[/]")
        rprint(f"   ğŸ’¥ {error}")


# ============================================================
# å‘½ä»¤å®šä¹‰
# ============================================================

@app.command("fetch")
def fetch(
    url: str = typer.Argument(..., help="çŸ¥ä¹é“¾æ¥ (é—®é¢˜/å›ç­”/ä¸“æ )"),
    output: Path = typer.Option(Path("./data"), "-o", "--output", help="è¾“å‡ºç›®å½•"),
    limit: Optional[int] = typer.Option(None, "-n", "--limit", help="é™åˆ¶å›ç­”æ•°é‡ (ä»…é™é—®é¢˜é¡µ)"),
    no_images: bool = typer.Option(False, "-i", "--no-images", help="ä¸ä¸‹è½½å›¾ç‰‡"),
    headless: bool = typer.Option(True, "-b", "--headless", help="æ— å¤´æ¨¡å¼è¿è¡Œæµè§ˆå™¨"),
) -> None:
    """
    æŠ“å–å•ä¸ªçŸ¥ä¹é“¾æ¥ã€‚

    ç¤ºä¾‹:
        zhihu fetch "https://www.zhihu.com/question/123456"
        zhihu fetch "https://www.zhihu.com/question/123456" -n 10
        zhihu fetch "https://www.zhihu.com/p/abcdef" -o ./output
    """
    log.info("fetch_started", url=url, limit=limit)

    try:
        downloader = ZhihuDownloader(url)

        # æ£€æµ‹æ˜¯å¦éœ€è¦ç™»å½•
        if not downloader.has_valid_cookies() and cfg.zhihu.cookies_required:
            rprint("[yellow]âš ï¸  æœªæ£€æµ‹åˆ°æœ‰æ•ˆ Cookieï¼Œå°†ä½¿ç”¨æ¸¸å®¢æ¨¡å¼[/yellow]")

        # å‡†å¤‡æŠ“å–é…ç½®
        scrape_config = {}
        if limit and "/question/" in url and "/answer/" not in url:
            scrape_config = {"start": 0, "limit": limit}

        # æ‰§è¡ŒæŠ“å–
        rprint(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")

        asyncio.run(_fetch_and_save(
            url=url,
            output_dir=output,
            scrape_config=scrape_config,
            download_images=not no_images,
            headless=headless
        ))

    except Exception as e:
        handle_error(e, log)
        raise SystemExit(1)


@app.command("batch")
def batch(
    input_file: Path = typer.Argument(..., help="URLåˆ—è¡¨æ–‡ä»¶ (æ¯è¡Œä¸€ä¸ªé“¾æ¥)"),
    output: Path = typer.Option(Path("./data"), "-o", "--output", help="è¾“å‡ºç›®å½•"),
    concurrency: int = typer.Option(4, "-c", "--concurrency", help="å¹¶å‘æ•°"),
    no_images: bool = typer.Option(False, "-i", "--no-images", help="ä¸ä¸‹è½½å›¾ç‰‡"),
    headless: bool = typer.Option(True, "-b", "--headless", help="æ— å¤´æ¨¡å¼"),
) -> None:
    """
    æ‰¹é‡æŠ“å–å¤šä¸ªçŸ¥ä¹é“¾æ¥ã€‚

    ç¤ºä¾‹:
        zhihu batch ./urls.txt
        zhihu batch ./urls.txt -c 8 -o ./output
    """
    if not input_file.exists():
        rprint(f"[red]âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}[/red]")
        raise SystemExit(1)

    urls = extract_urls(input_file.read_text())
    if not urls:
        rprint("[red]âŒ æœªæ‰¾åˆ°æœ‰æ•ˆé“¾æ¥[/red]")
        raise SystemExit(1)

    # é™åˆ¶å¹¶å‘æ•°
    max_concurrency = min(concurrency, len(urls), 8)
    log.info("batch_started", file=str(input_file), count=len(urls), concurrency=max_concurrency)
    rprint(f"[bold]ğŸ“‹ æ‰¹é‡ä»»åŠ¡: {len(urls)} ä¸ªé“¾æ¥ (å¹¶å‘: {max_concurrency})[/bold]")

    # å¹¶å‘æ‰§è¡Œ
    results = asyncio.run(_batch_concurrent(
        urls=urls,
        output_dir=output,
        concurrency=max_concurrency,
        download_images=not no_images,
        headless=headless
    ))

    # ç»Ÿè®¡ç»“æœ
    success = sum(1 for r in results if r["success"])
    failed = len(results) - success

    rprint(f"\n[bold]ğŸ“Š æ‰¹é‡å®Œæˆ: {success} æˆåŠŸ, {failed} å¤±è´¥[/bold]")
    log.info("batch_completed", success=success, failed=failed)


@app.command("config")
def config_cmd(
    show: bool = typer.Option(False, "--show", help="æ˜¾ç¤ºå½“å‰é…ç½®"),
    path: bool = typer.Option(False, "--path", help="æ˜¾ç¤ºé…ç½®æ–‡ä»¶è·¯å¾„"),
) -> None:
    """
    æŸ¥çœ‹æˆ–ç®¡ç†é…ç½®ã€‚

    ç¤ºä¾‹:
        zhihu config --show
        zhihu config --path
    """
    if path:
        from pathlib import Path as P
        config_path = P(__file__).parent.parent / "config.yaml"
        rprint(f"ğŸ“„ é…ç½®æ–‡ä»¶: [cyan]{config_path}[/]")
        raise SystemExit(0)

    if show:
        rprint(Panel(
            Text(f"""
[b]é…ç½®è·¯å¾„:[/] {Path(__file__).parent.parent / "config.yaml"}

[b]è¾“å‡ºç›®å½•:[/] {cfg.output.directory}
[b]æ—¥å¿—çº§åˆ«:[/] {cfg.logging.level}
[b]æµè§ˆå™¨:[/] {"æ— å¤´" if cfg.zhihu.browser.headless else "æœ‰å¤´"}
[b]é‡è¯•æ¬¡æ•°:[/] {cfg.crawler.retry.max_attempts}
[b]å›¾ç‰‡å¹¶å‘:[/] {cfg.crawler.images.concurrency}
            """.strip(), justify="left"),
            title="ğŸ› ï¸ å½“å‰é…ç½®",
            border_style="cyan"
        ))


@app.command("check")
def check() -> None:
    """
    æ£€æŸ¥ç¯å¢ƒä¾èµ–å’Œé…ç½®æ˜¯å¦æ­£å¸¸ã€‚
    """
    from playwright.async_api import async_playwright

    rprint("ğŸ” ç³»ç»Ÿæ£€æŸ¥...\n")

    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_path = Path(__file__).parent.parent / "config.yaml"
    if config_path.exists():
        rprint("âœ… config.yaml å­˜åœ¨")
    else:
        rprint("âŒ config.yaml ä¸å­˜åœ¨")

    # æ£€æŸ¥ cookies
    cookies_path = Path(__file__).parent.parent / "cookies.json"
    has_cookie = cookies_path.exists() and "YOUR_COOKIE_HERE" not in cookies_path.read_text()
    rprint(f"{'âœ…' if has_cookie else 'âš ï¸'} cookies.json {'æœ‰æ•ˆ' if has_cookie else 'æœªé…ç½®æˆ–æ— æ•ˆ'}")

    # æ£€æŸ¥ playwright
    try:
        asyncio.run(_check_playwright())
        rprint("âœ… Playwright æ­£å¸¸")
    except Exception as e:
        rprint(f"âŒ Playwright é”™è¯¯: {e}")


async def _check_playwright() -> None:
    """æ£€æŸ¥ playwright"""
    async with async_playwright() as pw:
        await pw.chromium.launch(headless=True)


# ============================================================
# å†…éƒ¨åŠ©æ‰‹
# ============================================================


async def _batch_concurrent(
    urls: List[str],
    output_dir: Path,
    concurrency: int,
    download_images: bool,
    headless: bool,
) -> List[Dict[str, Any]]:
    """
    å¹¶å‘æ‰¹é‡æŠ“å–

    Args:
        urls: URL åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        concurrency: å¹¶å‘æ•°
        download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        headless: æ— å¤´æ¨¡å¼

    Returns:
        ç»“æœåˆ—è¡¨
    """
    semaphore = asyncio.Semaphore(concurrency)
    humanizer = get_humanizer()

    async def fetch_one(url: str, index: int) -> Dict[str, Any]:
        async with semaphore:
            # ä»»åŠ¡é—´éšæœºå»¶è¿Ÿï¼Œé¿å…è§¦å‘åçˆ¬
            if index > 0:
                delay = uniform(0.5, 2.0) * (index % 3 + 1)
                await asyncio.sleep(delay)

            try:
                await _fetch_and_save(
                    url=url,
                    output_dir=output_dir,
                    scrape_config={},
                    download_images=download_images,
                    headless=headless
                )
                return {"url": url, "success": True}
            except Exception as e:
                handle_error(e, log)
                return {"url": url, "success": False, "error": str(e)}

    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
    tasks = [fetch_one(url, i) for i, url in enumerate(urls)]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # æ¸…ç†ç»“æœ
    cleaned = []
    for r in results:
        if isinstance(r, dict):
            cleaned.append(r)
        else:
            cleaned.append({"url": "unknown", "success": False})

    return cleaned


async def _fetch_and_save(
    url: str,
    output_dir: Path,
    scrape_config: dict,
    download_images: bool = True,
    headless: bool = True,
) -> None:
    """æ‰§è¡ŒæŠ“å–å¹¶ä¿å­˜"""
    from datetime import datetime

    downloader = ZhihuDownloader(url)
    data = await downloader.fetch_page(**scrape_config)

    if not data:
        rprint("[yellow]âš ï¸  æœªè·å–åˆ°å†…å®¹[/yellow]")
        return

    today = datetime.now().strftime("%Y-%m-%d")

    # å¤„ç†å•ä¸ªæˆ–å¤šä¸ªç»“æœ
    items = data if isinstance(data, list) else [data]

    for item in items:
        title = sanitize_filename(item["title"])
        author = sanitize_filename(item["author"])

        folder_name = f"[{today}] {title}"
        folder = output_dir / folder_name
        folder.mkdir(parents=True, exist_ok=True)

        # ä¸‹è½½å›¾ç‰‡
        img_map = {}
        if download_images:
            img_urls = ZhihuConverter.extract_image_urls(item["html"])
            if img_urls:
                rprint(f"   ğŸ“¥ ä¸‹è½½ {len(img_urls)} å¼ å›¾ç‰‡...")
                img_map = await ZhihuDownloader.download_images(
                    img_urls,
                    folder / "images"
                )

        # è½¬æ¢å¹¶ä¿å­˜
        converter = ZhihuConverter(img_map=img_map)
        md = converter.convert(item["html"])

        header = (
            f"# {item['title']}\n\n"
            f"> **ä½œè€…**: {item['author']}  \n"
            f"> **æ¥æº**: [{url}]({url})  \n"
            f"> **æ—¥æœŸ**: {today}\n\n"
            "---\n\n"
        )

        out_path = folder / "index.md"
        out_path.write_text(header + md, encoding="utf-8")

        rprint(f"âœ… ä¿å­˜: [cyan]{author}[/] - {title[:25]}...")
        rprint(f"   ğŸ“ {out_path}")


# ============================================================
# å…¥å£ç‚¹
# ============================================================

def main() -> None:
    """CLI å…¥å£"""
    app()


if __name__ == "__main__":
    main()