# 如何评价 BERT 模型？

> **作者**: 第欧根尼  
> **来源**: [https://www.zhihu.com/question/298203515/answer/2008258573281562692](https://www.zhihu.com/question/298203515/answer/2008258573281562692)  
> **日期**: 2026-02-21

---

当还是个小实验室的OpenAI用 GPT-1 证明了“Transformer + 预训练”可以横扫 NLP 任务时，Google 的研究员们感到了深深的刺痛：明明是我们发明的架构，为什么最好的效果被别人跑出来了？这里面感受最深的几个人中，包括了雅各布·德夫林（Jacob Devlin）。

![](https://pic1.zhimg.com/50/v2-55f868173e2b4198dc06860ce4c4f455_720w.jpg?source=2c26e567)

雅各布·德夫林（Jacob Devlin）

德夫林是作为NLP领域的专家加入Google的，但是他加入Google后却花了大量的时间在Google的基础设施中。他是 Google内部最早一批精通 TPU（张量处理器）编程的人。这样的双重背景让他得以发明一个混血怪物BERT(Bidirectional Encoder Representations from Transformers)，正是这头怪兽，将所有人的焦点从GPT-1返回到Google的身上。

BERT 一经发布，立刻在 11 项 NLP 任务上刷新了记录，把 GPT-1 刚创下的记录全部打破。GPT-1：72.8 分，BERT (Base)：79.6 分，BERT (Large)：80.5 分，距离人类的得分87分，只有不到十分的差距了。在 AI 学术圈，SOTA就是硬通货，当 BERT 在 GLUE 上把分差拉大到两位数时，所有做 NLP 的人都意识到：以前的方法都过时了。甚至有人还会激进的认为OpenAI 的“单向生成路线”可能是一条死胡同，而 Google 的“双向理解路线”才是通往 AGI 的正道。

BERT在Google内部的出现的故事，需要从Transformer在Google内部的处境开始。Transformer从一开始，就被视为是NLP领域的特种兵，专门用来做翻译的，没有任何人意识到这是一个可以改变整个领域格局的核武器。甚至它的八位作者，也都把精力投入在如何用它优化翻译上面了。其实这是典型的大公司KPI导向所造成的短视，Google 翻译团队的 KPI 是 BLEU 分数，Transformer在可以明确被证明可以优化这个分数的时候，自然所有的资源都会集中被投放到那里，而在其它方面的潜力就自然被忽略了。

在整个Google内部，“理解”的权重也远远大于“生成”。在Google对自己的定位上，它是全球最大的数字图书馆馆长。馆长的职责是什么？是把每一本书（网页）分门别类，读懂每一本书的内容，然后当用户来问问题时，精准地把那本包含答案的书递给用户。理解是馆长的核心能力，他必须理解书，也必须理解人。在 Google 看来，“生成”意味着创造新的信息，而这超出了“整合现有信息”的范畴。 Google 认为真理存在于网页中，它的任务是搬运真理，而不是制造真理。

所以，即使GPT-1出现之后，多数Transformer的作者都还没有开始认识到价值，而还是抱着这样的态度：“哦，OpenAI 用我们的架构刷了一些 NLP 任务的榜，挺好，证明我们的 Transformer 确实厉害。” —— 然后转头继续优化翻译去了。

德夫林是除了这几位作者之外，对Transformer最热衷的几个人之一。他当时负责优化各种 NLP 模型在 TPU 上的运行效率，Transformer 刚出来时，虽然理论很美，但代码非常难写，且很难在 TPU 上跑出最高效率。所以德夫林花了大量的时间对其进行优化。他是做机器翻译出身的，这种经验让他具有语言学的直觉，这种直觉一直让他觉得单向的 LSTM 或者单向的 Transformer Decoder 很别扭。在人类的阅读理解中，一个词的意思往往不仅仅取决于它前面的词，更取决于它后面的词。单向模型就像是一个被挡住了右眼的人，读到一个词时，根本不许看后面。在机器翻译的架构中，是分为Encoder（读）和 Decoder（写）的。Decoder 必须单向，在生成译文时，你确实不能偷看后面，因为后面的字还没写出来呢。这是物理限制，但是Encoder 不需要单向，对于等待翻译的源句子，这句中文是已经完整存在在那里的。这就是德夫林感到别扭的地方，明明给了一张全景地图，却非要卷成一个筒，通过一个小孔一点一点看，这就是人为的制造麻烦。

而GPT-1却更加大胆的把他觉得应该改造一下，会不那么别扭的部分直接砍掉了，只剩下不别扭但是在他来看也失去了核心理解能力的Decoder。他觉得有必要做点什么，“教训”一下OpenAI这帮胆大妄为的家伙。

德夫林的想法的确是更加符合语言学的逻辑，但是他那个想让Encoder能够全向可见的设想却在训练上遇到了困难。在NLP的训练过程中，公认的最佳的训练范式是“预测下一个词”。这种来自于香农，后来被本吉奥和OpenAI发扬光大的自回归语言建模，无论是在数据的无限性上，训练信号的稠密性上，还是接口的统一性上，都有着无以伦比的优势。再加上伊尔亚上升到哲学层面的“压缩即智能”，他认为虽然预测下一个词是结果，但为了达到这个结果，模型被迫学会智能。这使得“预测下一个词”是当前整个AGI路线中最优的训练范式。德夫林和他的BERT要挑战的就是这个范式。他的挑战，在当时来看，仿佛是成功的。

德夫林为了解决这个问题，发明了一个名为MLM (Masked Language Modeling，掩码语言模型)的训练范式。在把一句话喂给模型之前，BERT 会随机选中这句话中15%的词（Token），把它们“藏起来”。比如原始句子是`The cat sat on the mat` ，处理之后就会变成`The cat [MASK] on the mat.`

训练的目标就变成了：利用剩下没被遮住的词，猜出那个 `[MASK]` 到底代表什么词。

但是，真实世界的句子里是没有 `[MASK]` 这个符号的。为了更好的模拟现实世界，德夫林做了一个精妙的工程设计，对于那 15% 被选中的词，BERT 并不是全部替换成 `[MASK]`，而是8成替换，剩下的1成变成替换为一个随机词，而最后的1成则是保持原词不变。这样的设计使得MLM训练出的模型，果然在理解方面胜出GPT-1一筹。

和“预测下一个词”的范式背后有着“压缩即智能”的哲学思考一样，MLM的设计理念也蕴含着德夫林对认知的看法。MLM的本质是人为的对信号制造了噪声，而理解的本质就是“去躁”。在德夫林看来，理解是还原真相。世界充满了噪音和缺失（歧义、简写、错误），一个真正的智者，应该能在信息不完整、甚至有错误（随机词替换）的情况下，脑补出原本的完美世界。

这种认知洞见在CV领域，让何恺明推出了挖掉75%的图片块的MAE (Masked Autoencoders)，也直接让扩散模型（Diffusion Model）开始了用“去噪”来创造世界之路。无论是Stable Diffusion、DALL-E 3、Midjourney还是SORA，亦或者是特斯拉的FSD，都从德夫林的BERT中汲取了大量的营养。马斯克也公开承认，MLM所带来的“理解即去躁”是他们敢于抛弃雷达的第一性原理。

BERT 发布后，学术界发生了一种前所未有的现象：所有人都停止了手头的工作，开始研究 BERT。由于 BERT 的效果太好，但原理又太深奥，学术界甚至衍生出了一个专门的子学科叫 "BERTology"。成千上万篇论文涌现，专门研究“BERT 的哪个头（Head）学到了语法”、“哪一层学到了指代关系”。在那两年，NLP 只有两个方向：要么是魔改 BERT（如 RoBERTa, ALBERT, DistilBERT），要么是应用 BERT。所有的路都通向 Google。

2019 年，Google 宣布将 BERT 正式整合进 Google Search 算法中。加上 BERT 后，搜索引擎开始“理解人话”了。它能理解 “to”, “for” 这些介词在长句中微妙的逻辑关系。Google认为这是“Google 搜索历史上过去五年最大的飞跃”。

同时德夫林开源了BERT 的代码和预训练权重。 一夜之间，从阿里的淘宝搜索，到字节跳动的推荐算法，再到银行的客服机器人，全世界的 NLP 系统内核都换成了 BERT。Google 再次成为了基础设施的提供商，就像当年的 Android 一样。

在 2018 年底到 2020 年这大概两年的时间里，Google 凭借 BERT 重新确立了其在 AI 领域的绝对统治地位，也坚定的相信了AI 的终极形态是“完美的搜索引擎”，BERT 加固了 Google 的旧王座，却挡住了眺望新大陆的视线。BERT和GPT的竞争，不仅仅是工程层面的比拼，在底层更是一种思考模式的竞争，是德夫林和伊尔亚的两种不同哲学信仰的正面PK。

他们两个的比拼，形成了崇尚预测未来的GPT自回归（AR）阵营和认为智能的目的是还原真相的BERT自编码（AE）阵营。自回归是“时间的艺术”，自编码是“空间的艺术”，它们之间的竞争，仿佛让我们回到了CNN在空间领域称霸和RNN在时间领域跋涉的日子。往上追溯，似乎他们两个又分别进入了理性主义和经验主义的阵营，德夫林的去躁和苏格拉底的真理越辩越明是那么的相似，而伊尔亚的暴力出奇迹应会让洛克和培根（压缩即智能是归纳法的现代解读）大喊赞同。世界的所有具体分歧仿佛总会在哲学层面找到交汇处，人们在寻找智能的路途中也是如此。在现实中这些分歧的路线从来不会被认为决定胜负的关键因素，关键因素会隐藏在一些特别的具体事项中，往往竞赛者都会把精力集中于此。但是真正的智者早就从分歧的根源处找到判断的根据，他们能很清晰的看出来，什么样的补救是徒劳无功的，而什么样的方法才能真正解决问题。

MLM就有个这样致命的问题，就是它对训练数据的利用率很低，只能做到15%，这使得BERT的规模始终无法大幅提升，即使德夫林用尽浑身解术，把TPU的优化做到了极致，也没有太大的进展。而且在“挖洞”这个环节上，也会因为有些词有着前后关联（比如New York），而强行会对某些强关联的语义进行割裂，这导致BERT将天生无法在生成上达到GPT那样的丝滑程度。但是当时，这个关键问题被掩盖下来了，直到一个叫杨植麟的青年实习生口无遮拦的道破了它的破绽。杨植麟开始怀疑AE在路线上是否是最优，进而设计出了XLNET这样一个本质上是AR模型，却实现双向上下文能力的模型。XLNet 在当时横扫了 BERT 的所有榜单。这也让杨植麟相信，AR框架才是正统，AE 框架（BERT）只是为了双向性而走的歪路。 只要解决了双向性问题，AR 必胜。所以，几年后，杨植麟回到自己的祖国，推出了使用AR框架的Kimi。

![](https://pic1.zhimg.com/50/v2-3aa7092b85cea56d5886ba00c272ba15_720w.jpg?source=2c26e567)

杨植麟（Zhilin Yang）

BERT的另外一个问题，就是擅长理解，但是不擅长生成。科林·拉菲尔（Colin Raffel）带领团队把BERT丢弃的Decoder补了上来，这就是后来的T5模型。这个巨无霸模型迅速赶上了当时的GPT-2，但是由于完形填空式的训练方式，始终无法比得过只用Decoder的GPT，所以后来逐渐被Gemini所替代。但在当时，像 GPT 那样的生成式模型正在饱受幻觉的困扰，业界普遍认为正经工作用BERT，一些创造性高，对幻觉容忍度强的才可以用GPT，T5向行业证明了生成式模型也可以做正经工作，只需要做大规模的有监督微调。一时之间，Google建立起来压倒性的优势，但是大家却不那么清楚为了换得这暂时的优势，Google内部付出了多大的代价。为了找到最好的方案，当时T5团队控制变量，把当时 NLP 界几乎所有的技巧（不同的预训练目标、不同的掩码策略、不同的架构、不同的数据集）全都在 TPU 上跑了一遍，在数据清洗上，从数以 PB 计的原始数据中，最后只提炼出了约 750GB 的高质量数据。在几乎无上限的资源支撑下，一个11B的T5才对当时才1.5B的GPT-2形成了碾压。

![](https://pic1.zhimg.com/50/v2-bfc852c6a3715be80c8d3f852d380c61_720w.jpg?source=2c26e567)

科林·拉菲尔（Colin Raffel）

T5的力大砖飞刺激了OpenAI，OpenAI被迫放弃了与 BERT和T5 在 GLUE 榜单上的纠缠，埋头开始堆数据。伊尔亚的工程狂魔特性加上奥特曼巧舌如簧带来的源源不断的资源，OpenAI 在 2020 年发布了著名的《Scaling Laws for Neural Language Models》论文。在这篇同样著名的论文中，他们证明了自回归模型的性能与参数量、数据量、计算量呈严格的幂律关系，只要敢堆算力，GPT 的智力就敢一直涨，看不到尽头。而 BERT 类模型在达到一定规模后，收益就会开始递减。

OpenAI并不是只发论文，他们很快发布了1750 亿参数的 GPT-3。Google也考虑过继续把BERT做大，但那挖洞训练范式的信号密度不够的问题开始制约模型规模的扩大。MLM只能利用其中15%的训练数据，这使得模型扩大到 1000 亿参数时，训练成本是天文数字。训练一个 175B 的 BERT，可能要比训练一个 175B 的 GPT 贵好几倍，而且收敛更慢。这就是工业界目前看到的基于BERT的模型，最大规模的也就是Google 的 T5-11B 或 Megatron-BERT的原因。而自回归模型在现在都已经突破了万亿的参数规模，在绝对的规模面前，精巧的结构失效了。力大砖飞才似乎是AI世界的真理。

在那以后，BERT彻底退出了“通用大模型”的争夺战，Google也随着BERT的沉沦，开始显露出明显的焦虑，那段时间昏招频出。这些昏招让德夫林觉得无法忍受，毅然在ChatGPT发布两个月后加入了OpenAI。直到2023 年 4 月，Google 终于下定决心，将 Google Brain和 DeepMind 合并，成立了全新的 Google DeepMind，由 戴密斯·哈萨比斯（Demis Hassabis） 统一领导，集全公司之力打造 Gemini。这种“集中力量办大事”的决心，以及 Gemini 项目宏大的愿景，重新吸引了德夫林。他选择回归，成为 Gemini 开发团队的核心成员。但此时Google手中的Gemini，已然是自回归模型了。

但BERT的故事并没有结束，后面我们还会继续遇到浴火重生的它。

哈萨比斯则开启了另外一段精彩无比的故事和一系列同样令人惊叹的对智能本质的思考。已经根深蒂固，固若磐石的连接主义底盘，在哈萨比斯登上舞台之后，开始出现了一些晃动。

这是专栏中的节选，更多内容，请看

[硅基物种起源](https://www.zhihu.com/column/c_2000233132604031127?share_code=iOdjOfFAL74A&utm_psn=2008258422164977217)
