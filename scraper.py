"""
scraper.py â€” çŸ¥ä¹é¡µé¢æŠ“å– & å›¾ç‰‡ä¸‹è½½æ¨¡å—

å…è´£å£°æ˜ï¼š
æœ¬é¡¹ç›®ä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œå­¦ä¹ äº¤æµä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚
ä½¿ç”¨è€…åº”éµå®ˆçŸ¥ä¹çš„ç›¸å…³æœåŠ¡åè®®å’Œ robots.txt åè®®ã€‚
å› ä½¿ç”¨æœ¬é¡¹ç›®ä»£ç è€Œäº§ç”Ÿçš„ä»»ä½•æ³•å¾‹çº çº·æˆ–åæœï¼Œç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…ã€‚

é›†æˆ MediaCrawler çš„åçˆ¬ç­–ç•¥ï¼šPersistent Context, Stealth JS, WebGL Mock, Proxy.
"""

import asyncio
import hashlib
import json
import re
from pathlib import Path
from urllib.parse import urlparse
from typing import Union, List, Optional

import httpx
# éœ€ pip install PyExecJS
import execjs
from playwright.async_api import async_playwright, Playwright
import subprocess

def get_auto_proxy() -> Optional[str]:
    """
    è‡ªåŠ¨è·å– macOS ç³»ç»Ÿä»£ç†è®¾ç½® (Shadowrocket/ClashX)ã€‚
    è§£æ `scutil --proxy` çš„è¾“å‡ºã€‚
    """
    try:
        output = subprocess.check_output("scutil --proxy", shell=True).decode("utf-8")
        if "HTTPEnable : 1" in output:
            # æå–ç«¯å£
            match = re.search(r"HTTPPort : (\d+)", output)
            if match:
                port = match.group(1)
                print(f"âœ… å·²è‡ªåŠ¨æ£€æµ‹åˆ°ç³»ç»Ÿä»£ç†: http://127.0.0.1:{port}")
                return f"http://127.0.0.1:{port}"
    except Exception:
        pass
    
    print("âš ï¸  æœªæ£€æµ‹åˆ°ç³»ç»Ÿä»£ç†ï¼Œå°è¯•ç›´è¿...")
    return None
# å…¨å±€é…ç½®
# è‡ªåŠ¨æ£€æµ‹æœ¬åœ°ä»£ç† (127.0.0.1:xxxx)
PROXY_SERVER = get_auto_proxy()
USER_DATA_DIR = Path(__file__).parent / "browser_data"
STEALTH_JS_PATH = Path(__file__).parent / "stealth.min.js"
ZHIHU_JS_PATH = Path(__file__).parent / "libs" / "z_core.js"


class ZhihuDownloader:
    """ä»çŸ¥ä¹æ–‡ç« /å›ç­”é¡µé¢æŠ“å– HTML å†…å®¹å¹¶ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°ã€‚"""

    _UA = (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/128.0.0.0 Safari/537.36"
    )

    _IMG_HEADERS = {
        "Referer": "https://www.zhihu.com/",
        "User-Agent": _UA,
    }

    def __init__(self, url: str) -> None:
        self.url = url.split("?")[0]
        self.page_type = self._detect_type()
        self._js_ctx = self._init_js_context()

    def _detect_type(self) -> str:
        if "zhuanlan.zhihu.com" in self.url:
            return "article"
        if "/answer/" in self.url:
            return "answer"
        if "/question/" in self.url:
            return "question"
        return "article"

    def _init_js_context(self):
        """åˆå§‹åŒ– JS æ‰§è¡Œç¯å¢ƒ (å¤‡ç”¨ï¼Œç”¨äºç”Ÿæˆ x-zse-96)ã€‚"""
        if ZHIHU_JS_PATH.exists():
            try:
                with open(ZHIHU_JS_PATH, "r", encoding="utf-8") as f:
                    js_code = f.read()
                return execjs.compile(js_code)
            except Exception as e:
                print(f"âš ï¸  JS ç¯å¢ƒåŠ è½½å¤±è´¥: {e}")
        return None

    def _get_signature(self, url: str) -> dict:
        """ç”Ÿæˆ x-zse-96 ç­¾åã€‚"""
        if not self._js_ctx:
            return {}
        try:
            # æå– path, e.g. /question/xxx
            path = urlparse(url).path
            # å‰é¢åˆ¤ç©ºäº† self._js_ctxï¼Œè¿™é‡Œæ˜¾å¼ç±»å‹æ–­è¨€æˆ–ç›´æ¥è°ƒç”¨
            from typing import cast, Any
            ctx = cast(Any, self._js_ctx)
            return ctx.call("get_sign", path, "d_c0=SEARCH_ME") # d_c0 is simplified
        except Exception as e:
            # print(f"âš ï¸  ç­¾åç”Ÿæˆå¤±è´¥: {e}")
            return {}

    # â”€â”€ é¡µé¢æŠ“å– Core â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _load_cookies(self) -> List[dict]:
        """ä» cookies.json åŠ è½½ Cookieã€‚è¿‡æ»¤æ‰å ä½ç¬¦ã€‚"""
        cookie_path = Path(__file__).parent / "cookies.json"
        if cookie_path.exists():
            try:
                with open(cookie_path, "r", encoding="utf-8") as f:
                    cookies = json.load(f)
                    # è¿‡æ»¤æ‰å¸¦æœ‰å ä½ç¬¦çš„ Cookie
                    valid_cookies = [
                        c for c in cookies 
                        if c.get("value") and c.get("value") != "YOUR_COOKIE_HERE"
                    ]
                    return valid_cookies
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ cookies.json å¤±è´¥: {e}")
        return []

    async def fetch_page(self, **kwargs) -> Union[dict, List[dict]]:
        """
        ä½¿ç”¨ Persistent Context + Stealth + Proxy æŠ“å–é¡µé¢ã€‚
        æ”¯æŒä¼ å…¥ kwargs (å¦‚ start, limit) ä¼ é€’ç»™ _extract_questionã€‚
        """
        async with async_playwright() as pw:
            # å‡†å¤‡å¯åŠ¨å‚æ•°
            launch_args = [
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-infobars",
                "--disable-gpu",  # æœ‰æ—¶ç¦ç”¨ GPU æ›´ç¨³å®š
            ]

            # å¯åŠ¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡
            context = await pw.chromium.launch_persistent_context(
                user_data_dir=USER_DATA_DIR,
                headless=True,  # ä¾ç„¶å¯ä»¥ç”¨ Headlessï¼Œé…åˆ Stealth
                args=launch_args,
                user_agent=self._UA,
                viewport={"width": 1920, "height": 1080},
                proxy={"server": PROXY_SERVER} if PROXY_SERVER else None,
                java_script_enabled=True,
                locale="zh-CN",
                channel="chrome",  # å°è¯•ç”¨æœ¬æœº Chrome
            )

            try:
                page = context.pages[0] if context.pages else await context.new_page()

                # 1. æ³¨å…¥ stealth.min.js
                if STEALTH_JS_PATH.exists():
                    await context.add_init_script(path=STEALTH_JS_PATH)
                else:
                    print("âš ï¸  æœªæ‰¾åˆ° stealth.min.jsï¼Œåçˆ¬èƒ½åŠ›å¯èƒ½ä¸‹é™")

                # 1.5 æ³¨å…¥ Cookies
                cookies = self._load_cookies()
                if cookies:
                    await context.add_cookies(cookies)
                    print(f"ğŸª å·²åŠ è½½ {len(cookies)} ä¸ª Cookie")

                # 2. æ³¨å…¥é¢å¤–çš„ WebGL / Navigator ä¼ªé€  (å‚è€ƒ MediaCrawler CDP)
                await context.add_init_script("""
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                    
                    // ä¼ªé€  WebGL
                    const getParameter = WebGLRenderingContext.prototype.getParameter;
                    WebGLRenderingContext.prototype.getParameter = function(parameter) {
                        if (parameter === 37445) return 'Intel Inc.';
                        if (parameter === 37446) return 'Intel Iris OpenGL Engine';
                        return getParameter(parameter);
                    };
                """)

                # æ³¨å…¥ç­¾å (å¦‚æœæ˜¯ API è¯·æ±‚ï¼Œè¿™é‡Œä¸»è¦æ¼”ç¤ºæ€è·¯ï¼Œå®é™…é¡µé¢è®¿é—®ä¸éœ€è¦æ‰‹åŠ¨åŠ  Headerï¼Œæµè§ˆå™¨ä¼šè‡ªåŠ¨å¤„ç†)
                # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠç­¾ååŠ åˆ° extraHTTPHeaders
                sig = self._get_signature(self.url)
                if sig:
                   await page.set_extra_http_headers(sig)

                # 3. è®¾ç½®é»˜è®¤ Timeout
                page.set_default_timeout(30000)

                # 4. è®¿é—®é¡µé¢
                print(f"ğŸŒ è®¿é—®: {self.url}")
                # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸäºº
                await asyncio.sleep(1)
                
                await page.goto(self.url, wait_until="domcontentloaded")
                
                # ç­‰å¾… JS æ‰§è¡Œå’Œåçˆ¬æ£€æµ‹é€šè¿‡
                await page.wait_for_timeout(3000)

                # 5. å¤„ç†å¼¹çª—
                await self._dismiss_popup(page)

                # 6. æå–å†…å®¹
                # 6. æå–å†…å®¹
                if self.page_type == "article":
                    result = await self._extract_article(page)
                elif self.page_type == "question":
                    result = await self._extract_question(page, **kwargs)
                else:
                    result = await self._extract_answer(page)

                return result

            finally:
                await context.close()

    async def _dismiss_popup(self, page) -> None:
        """å…³é—­ç™»å½•å¼¹çª—ã€‚"""
        try:
            btn = page.locator("button.Modal-closeButton")
            if await btn.count() > 0:
                await btn.click(timeout=2000)
                await page.wait_for_timeout(500)
        except Exception:
            pass

    async def _extract_article(self, page) -> dict:
        """æå–æ–‡ç« ã€‚"""
        # çŸ¥ä¹çš„åçˆ¬æœ‰æ—¶ä¼šè¿”å› JSON é”™è¯¯
        text = await page.locator("body").inner_text()
        if "40362" in text or "è¯·æ±‚å­˜åœ¨å¼‚å¸¸" in text:
            raise Exception("è§¦å‘çŸ¥ä¹åçˆ¬ (40362)")

        # ç­‰å¾…æ ‡é¢˜
        await page.wait_for_selector("h1.Post-Title", timeout=10000)

        title = await page.locator("h1.Post-Title").inner_text()
        author = await self._safe_text(
            page, ".AuthorInfo span.UserLink-Name", "æœªçŸ¥ä½œè€…"
        )
        if author == "æœªçŸ¥ä½œè€…":
            author = await self._safe_text(
                page, ".AuthorInfo-name .UserLink-link", "æœªçŸ¥ä½œè€…"
            )
        
        date = await self._extract_date(page)

        # ä¼˜å…ˆç”±å®¹å™¨æ‰¾
        rich = page.locator(".Post-RichTextContainer .RichText").first
        if await rich.count() > 0:
            html = await rich.inner_html()
        else:
            html = await page.locator(".RichText").first.inner_html()

        # å°è¯•è·å–å¤´å›¾
        try:
            title_img = page.locator("img.TitleImage").first
            if await title_img.count() > 0:
                src = await title_img.get_attribute("src")
                if src:
                    html = f'<img src="{src}" alt="TitleImage"><br>{html}'
        except Exception:
            pass

        return {"title": title.strip(), "author": author.strip(), "html": html, "date": date}

    async def _extract_question(self, page, start: int = 0, limit: int = 3) -> List[dict]:
        """
        æå–é—®é¢˜ä¸‹çš„å¤šä¸ªå›ç­”ã€‚
        :param start: ä»ç¬¬å‡ ä¸ªå›ç­”å¼€å§‹æŠ“ (0-indexed)
        :param limit: æŠ“å–å¤šå°‘ä¸ª (é»˜è®¤ 3 ä¸ª)
        """
        text = await page.locator("body").inner_text()
        if "40362" in text or "è¯·æ±‚å­˜åœ¨å¼‚å¸¸" in text:
            raise Exception("è§¦å‘çŸ¥ä¹åçˆ¬ (40362)")

        # ç­‰å¾…é—®é¢˜æ ‡é¢˜åŠ è½½
        try:
            await page.wait_for_selector(".QuestionHeader-title", timeout=5000)
        except:
            pass
        
        # å°è¯•ç‚¹å‡» "æŸ¥çœ‹å…¨éƒ¨" æŒ‰é’® (å¦‚æœæ˜¯ auto æ¨¡å¼ä¸” limit è¾ƒå°ï¼Œå…¶å®å¯ä»¥ä¸ç‚¹ï¼Œä¸ºäº†ä¿é™©è¿˜æ˜¯ç‚¹ä¸€ä¸‹)
        await self._click_view_all(page)

        # ç­‰å¾…è‡³å°‘ä¸€ä¸ªå›ç­”é¡¹åŠ è½½
        try:
            await page.wait_for_selector(".ContentItem.AnswerItem", timeout=5000)
        except:
            print("âš ï¸ æœªæ£€æµ‹åˆ°å›ç­”åˆ—è¡¨ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–æ— å›ç­”")

        # æ™ºèƒ½æ»šåŠ¨é€»è¾‘
        target_count = start + limit
        print(f"ğŸ¯ ç›®æ ‡: æŠ“å–å‰ {target_count} ä¸ªå›ç­”")

        prev_count = 0
        max_scroll_attempts = 30  # ç¨å¾®å‡å°‘å°è¯•æ¬¡æ•°ï¼Œé¿å…æ­»å¾ªç¯
        no_change_count = 0

        while True:
            answers = page.locator(".ContentItem.AnswerItem")
            count = await answers.count()
            print(f"ğŸ”„ å½“å‰åŠ è½½äº† {count} ä¸ªå›ç­”...")

            if count >= target_count:
                break
            
            if count == prev_count:
                no_change_count += 1
                if no_change_count >= 3: # 3æ¬¡æ²¡åŠ¨é™å°±åœï¼Œæ›´çµæ•
                    print("âš ï¸  å·²æ»šåŠ¨åˆ°åº•éƒ¨æˆ–æ— æ³•åŠ è½½æ›´å¤š")
                    break
            else:
                no_change_count = 0
            
            prev_count = count
            
            # æ»šåŠ¨
            await page.mouse.wheel(0, 10000)
            await asyncio.sleep(0.5)
            await page.keyboard.press("End")
            await asyncio.sleep(1.0)
            
            max_scroll_attempts -= 1
            if max_scroll_attempts <= 0:
                print("âš ï¸  è¾¾åˆ°æœ€å¤§æ»šåŠ¨æ¬¡æ•°")
                break
        
        # è·å–æ‰€æœ‰å›ç­”å¡ç‰‡
        answers = page.locator(".ContentItem.AnswerItem")
        total_found = await answers.count()
        print(f"ğŸ“Š å…±å‘ç° {total_found} ä¸ªå›ç­”ï¼Œå‡†å¤‡æå–èŒƒå›´ [{start}:{target_count}]...")
        
        results = []
        actual_limit = min(total_found, target_count)
        
        # è·å–é—®é¢˜æ ‡é¢˜ (é€šç”¨)
        question_title = await self._safe_text(page, "h1.QuestionHeader-title", "æœªçŸ¥é—®é¢˜")

        for i in range(start, actual_limit):
            item = answers.nth(i)
            try:
                data = await self._parse_answer_element(item, page, question_title)
                results.append(data)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ç¬¬ {i+1} ä¸ªå›ç­”: {e}")
        
        return results

    async def _click_view_all(self, page):
        """ç‚¹å‡»â€˜æŸ¥çœ‹å…¨éƒ¨â€™æŒ‰é’®çš„å°è£…ã€‚"""
        try:
            view_all_btn = page.get_by_text("æŸ¥çœ‹å…¨éƒ¨")
            if await view_all_btn.count() > 0:
                print("ğŸ‘† å‘ç° 'æŸ¥çœ‹å…¨éƒ¨' æŒ‰é’®ï¼Œå°è¯•ç‚¹å‡»...")
                await view_all_btn.first.click()
                await asyncio.sleep(2)
            else:
                 view_all_btn_alt = page.locator(".QuestionMainAction")
                 if await view_all_btn_alt.count() > 0:
                     print("ğŸ‘† å‘ç° '.QuestionMainAction' æŒ‰é’®ï¼Œå°è¯•ç‚¹å‡»...")
                     await view_all_btn_alt.first.click()
                     await asyncio.sleep(2)
                 else:
                    btns = page.locator("button")
                    count = await btns.count()
                    for i in range(count):
                        txt = await btns.nth(i).inner_text()
                        if "æŸ¥çœ‹å…¨éƒ¨" in txt or "View All" in txt:
                            print(f"ğŸ‘† å‘ç°æŒ‰é’® '{txt}'ï¼Œå°è¯•ç‚¹å‡»...")
                            await btns.nth(i).click()
                            await asyncio.sleep(2)
                            break
        except Exception as e:
            print(f"âš ï¸  ç‚¹å‡» 'æŸ¥çœ‹å…¨éƒ¨' æŒ‰é’®å¤±è´¥æˆ–æ— éœ€ç‚¹å‡»: {e}")

    async def _extract_answer(self, page) -> dict:
        """æå–å•ä¸ªå›ç­”ã€‚"""
        text = await page.locator("body").inner_text()
        if "40362" in text:
            raise Exception("è§¦å‘çŸ¥ä¹åçˆ¬ (40362)")

        # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œæ”¹ç”¨æ›´å®½æ³›çš„é€‰æ‹©å™¨ï¼Œé¿å… strictly waiting for .QuestionAnswer-content
        try:
            # ä¼˜å…ˆç­‰å¾…å›ç­”ä¸»ä½“ï¼Œç»™ 15s è¶…æ—¶
            await page.wait_for_selector(".ContentItem.AnswerItem", timeout=15000)
        except:
            print("âš ï¸  ç­‰å¾…å›ç­”å†…å®¹è¶…æ—¶ï¼Œå°è¯•ç›´æ¥è§£æ...")
        
        # å°è¯•ä» URL æå– answer_id
        answer_id = None
        match = re.search(r"answer/(\d+)", self.url)
        if match:
            answer_id = match.group(1)
            
        # ç¡®å®šå†…å®¹å®¹å™¨
        container = page.locator(".ContentItem.AnswerItem").first
        
        if answer_id:
            # å°è¯•ç²¾ç¡®å®šä½
            specific_item = page.locator(f".ContentItem.AnswerItem[name='{answer_id}']")
            if await specific_item.count() > 0:
                print(f"ğŸ¯ å®šä½åˆ°æŒ‡å®šå›ç­”: {answer_id}")
                container = specific_item.first
            else:
                zop_item = page.locator(f".ContentItem.AnswerItem[data-zop*='{answer_id}']")
                if await zop_item.count() > 0:
                    print(f"ğŸ¯ é€šè¿‡ data-zop å®šä½åˆ°æŒ‡å®šå›ç­”: {answer_id}")
                    container = zop_item.first
        
        # è·å–é—®é¢˜æ ‡é¢˜
        question_title = await self._safe_text(page, "h1.QuestionHeader-title", "æœªçŸ¥é—®é¢˜")
        
        return await self._parse_answer_element(container, page, question_title)

    async def _parse_answer_element(self, element, page, question_title) -> dict:
        """è§£æå•ä¸ªå›ç­”å…ƒç´ """
        # ä½œè€…
        author = await self._safe_text(element, ".AuthorInfo-name .UserLink-link", "æœªçŸ¥ä½œè€…")
        if author == "æœªçŸ¥ä½œè€…":
            author = await self._safe_text(element, ".AuthorInfo span.UserLink-Name", "æœªçŸ¥ä½œè€…")
        
        # èµåŒæ•°
        upvotes_text = await self._safe_text(element, "button.VoteButton--up", "0")
        # æå–æ•°å­—, e.g. "èµåŒ 1.2 ä¸‡" -> 12000
        upvotes = self._parse_upvotes(upvotes_text)

        # å‘å¸ƒæ—¶é—´
        date = await self._extract_date(element)
        
        # å†…å®¹ HTML
        rich = element.locator(".RichText").first
        if await rich.count() > 0:
            html = await rich.inner_html()
        else:
             html = "<p>æ— æ³•è·å–å†…å®¹</p>"

        return {
            "title": question_title.strip(), 
            "author": author.strip(), 
            "html": html, 
            "date": date,
            "upvotes": upvotes
        }

    def _parse_upvotes(self, text: str) -> int:
        """è§£æèµåŒæ•°æ–‡æœ¬ã€‚"""
        # e.g. "èµåŒ 1,234", "1.2 ä¸‡", "750"
        m = re.search(r"([\d\.,]+)\s*([ä¸‡kK]?)", text)
        if not m: return 0
        num_str = m.group(1).replace(",", "")
        unit = m.group(2).lower()
        try:
            val = float(num_str)
            if unit == "ä¸‡": val *= 10000
            elif unit in ("k", "K"): val *= 1000
            return int(val)
        except:
            return 0

    async def _extract_date(self, element) -> str:
        from datetime import date as dt_date
        try:
            # 1. å°è¯•æ‰¾ meta (é€‚ç”¨äº Page æˆ–åŒ…å« meta çš„å®¹å™¨)
            meta = await element.locator('meta[itemprop="datePublished"]').get_attribute("content", timeout=500)
            if meta: return meta[:10]
        except: pass
        
        try:
            # 2. å°è¯•æ‰¾ "å‘å¸ƒäº ..." æ–‡æœ¬ (é€‚ç”¨äº AnswerItem)
            text = await element.locator(".ContentItem-time").first.inner_text(timeout=500)
            m = re.search(r"(\d{4}-\d{2}-\d{2})", text)
            if m: return m.group(1)
        except: pass

        return dt_date.today().isoformat()

    async def _safe_text(self, page, selector: str, default: str) -> str:
        try:
            el = page.locator(selector).first
            return await el.inner_text(timeout=2000)
        except:
            return default

    # â”€â”€ å›¾ç‰‡ä¸‹è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @classmethod
    async def download_images(cls, img_urls: List[str], dest: Path) -> dict[str, str]:
        dest.mkdir(parents=True, exist_ok=True)
        url_to_local: dict[str, str] = {}

        # é…ç½®ä»£ç†
        limits = httpx.Limits(max_keepalive_connections=5, max_connections=10)
        async with httpx.AsyncClient(
            headers=cls._IMG_HEADERS,
            timeout=30.0,
            follow_redirects=True,
            proxy=PROXY_SERVER,  # å›¾ç‰‡ä¸‹è½½ä¹Ÿèµ°ä»£ç†
            limits=limits,
        ) as client:
            tasks = [cls._download_one(client, url, dest, url_to_local) for url in img_urls]
            await asyncio.gather(*tasks, return_exceptions=True)

        return url_to_local

    @staticmethod
    async def _download_one(client, url, dest, mapping):
        try:
            if url.startswith("//"): url = "https:" + url
            
            # è¿‡æ»¤ä¸éœ€è¦çš„å›¾ç‰‡
            if "data:image" in url or "equation" in url:
                return

            resp = await client.get(url)
            resp.raise_for_status()

            ext = Path(urlparse(url).path).suffix or ".jpg"
            if len(ext) > 5: ext = ".jpg"
            
            fname = hashlib.md5(url.encode()).hexdigest()[:12] + ext
            fpath = dest / fname

            fpath.write_bytes(resp.content)
            # å¿…é¡»ç”¨ / åˆ†éš”ï¼Œè¦åœ¨ Markdown é‡Œç”¨
            mapping[url] = f"images/{fname}"
        except Exception:
            pass