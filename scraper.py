"""
scraper.py â€” çŸ¥ä¹é¡µé¢æŠ“å– & å›¾ç‰‡ä¸‹è½½æ¨¡å—
é›†æˆ MediaCrawler çš„åçˆ¬ç­–ç•¥ï¼šPersistent Context, Stealth JS, WebGL Mock, Proxy.
"""

import asyncio
import hashlib
import json
import re
from pathlib import Path
from urllib.parse import urlparse

import httpx
# éœ€ pip install PyExecJS
import execjs
from playwright.async_api import async_playwright, Playwright

# å…¨å±€é…ç½®
# å¦‚æœæœ¬åœ° Shadowrocket/Clash å¼€å¯äº† 1087 ç«¯å£ï¼Œè¯·ä½¿ç”¨ä¸‹é¢çš„é…ç½®ï¼›å¦åˆ™è®¾ä¸º None
# å¦‚æœæœ¬åœ° Shadowrocket/Clash å¼€å¯äº† 1087 ç«¯å£ï¼Œè¯·ä½¿ç”¨ "http://127.0.0.1:1087"ï¼›å¦åˆ™è®¾ä¸º None
PROXY_SERVER = "http://127.0.0.1:1082"
USER_DATA_DIR = Path(__file__).parent / "browser_data"
STEALTH_JS_PATH = Path(__file__).parent / "stealth.min.js"
ZHIHU_JS_PATH = Path(__file__).parent / "zhihu.js"


class ZhihuDownloader:
    """ä»çŸ¥ä¹æ–‡ç« /å›ç­”é¡µé¢æŠ“å– HTML å†…å®¹å¹¶ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°ã€‚"""

    _UA = (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/128.0.0.0 Safari/537.36"
    )

    _IMG_HEADERS = {
        "Referer": "https://www.zhihu.com/",
        "User-Agent": _UA,
    }

    def __init__(self, url: str) -> None:
        self.url = url.split("?")[0]
        self.page_type = self._detect_type()
        self._js_ctx = self._init_js_context()

    def _detect_type(self) -> str:
        if "zhuanlan.zhihu.com" in self.url:
            return "article"
        if "/answer/" in self.url:
            return "answer"
        return "article"

    def _init_js_context(self):
        """åˆå§‹åŒ– JS æ‰§è¡Œç¯å¢ƒ (å¤‡ç”¨ï¼Œç”¨äºç”Ÿæˆ x-zse-96)ã€‚"""
        if ZHIHU_JS_PATH.exists():
            try:
                with open(ZHIHU_JS_PATH, "r", encoding="utf-8") as f:
                    js_code = f.read()
                return execjs.compile(js_code)
            except Exception as e:
                print(f"âš ï¸  JS ç¯å¢ƒåŠ è½½å¤±è´¥: {e}")
        return None

    def _get_signature(self, url: str) -> dict:
        """ç”Ÿæˆ x-zse-96 ç­¾åã€‚"""
        if not self._js_ctx:
            return {}
        try:
            # æå– path, e.g. /question/xxx
            path = urlparse(url).path
            # å‰é¢åˆ¤ç©ºäº† self._js_ctxï¼Œè¿™é‡Œæ˜¾å¼ç±»å‹æ–­è¨€æˆ–ç›´æ¥è°ƒç”¨
            from typing import cast, Any
            ctx = cast(Any, self._js_ctx)
            return ctx.call("get_sign", path, "d_c0=SEARCH_ME") # d_c0 is simplified
        except Exception as e:
            # print(f"âš ï¸  ç­¾åç”Ÿæˆå¤±è´¥: {e}")
            return {}

    # â”€â”€ é¡µé¢æŠ“å– Core â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def fetch_page(self) -> dict:
        """
        ä½¿ç”¨ Persistent Context + Stealth + Proxy æŠ“å–é¡µé¢ã€‚
        """
        async with async_playwright() as pw:
            # å‡†å¤‡å¯åŠ¨å‚æ•°
            launch_args = [
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-infobars",
                "--disable-gpu",  # æœ‰æ—¶ç¦ç”¨ GPU æ›´ç¨³å®š
            ]

            # å¯åŠ¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡
            context = await pw.chromium.launch_persistent_context(
                user_data_dir=USER_DATA_DIR,
                headless=True,  # ä¾ç„¶å¯ä»¥ç”¨ Headlessï¼Œé…åˆ Stealth
                args=launch_args,
                user_agent=self._UA,
                viewport={"width": 1920, "height": 1080},
                proxy={"server": PROXY_SERVER} if PROXY_SERVER else None,
                java_script_enabled=True,
                locale="zh-CN",
                channel="chrome",  # å°è¯•ç”¨æœ¬æœº Chrome
            )

            try:
                page = context.pages[0] if context.pages else await context.new_page()

                # 1. æ³¨å…¥ stealth.min.js
                if STEALTH_JS_PATH.exists():
                    await context.add_init_script(path=STEALTH_JS_PATH)
                else:
                    print("âš ï¸  æœªæ‰¾åˆ° stealth.min.jsï¼Œåçˆ¬èƒ½åŠ›å¯èƒ½ä¸‹é™")

                # 2. æ³¨å…¥é¢å¤–çš„ WebGL / Navigator ä¼ªé€  (å‚è€ƒ MediaCrawler CDP)
                await context.add_init_script("""
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                    
                    // ä¼ªé€  WebGL
                    const getParameter = WebGLRenderingContext.prototype.getParameter;
                    WebGLRenderingContext.prototype.getParameter = function(parameter) {
                        if (parameter === 37445) return 'Intel Inc.';
                        if (parameter === 37446) return 'Intel Iris OpenGL Engine';
                        return getParameter(parameter);
                    };
                """)

                # æ³¨å…¥ç­¾å (å¦‚æœæ˜¯ API è¯·æ±‚ï¼Œè¿™é‡Œä¸»è¦æ¼”ç¤ºæ€è·¯ï¼Œå®é™…é¡µé¢è®¿é—®ä¸éœ€è¦æ‰‹åŠ¨åŠ  Headerï¼Œæµè§ˆå™¨ä¼šè‡ªåŠ¨å¤„ç†)
                # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠç­¾ååŠ åˆ° extraHTTPHeaders
                sig = self._get_signature(self.url)
                if sig:
                   await page.set_extra_http_headers(sig)

                # 3. è®¾ç½®é»˜è®¤ Timeout
                page.set_default_timeout(30000)

                # 4. è®¿é—®é¡µé¢
                print(f"ğŸŒ è®¿é—®: {self.url}")
                # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸäºº
                await asyncio.sleep(1)
                
                await page.goto(self.url, wait_until="domcontentloaded")
                
                # ç­‰å¾… JS æ‰§è¡Œå’Œåçˆ¬æ£€æµ‹é€šè¿‡
                await page.wait_for_timeout(3000)

                # 5. å¤„ç†å¼¹çª—
                await self._dismiss_popup(page)

                # 6. æå–å†…å®¹
                if self.page_type == "article":
                    result = await self._extract_article(page)
                else:
                    result = await self._extract_answer(page)

                return result

            finally:
                await context.close()

    async def _dismiss_popup(self, page) -> None:
        """å…³é—­ç™»å½•å¼¹çª—ã€‚"""
        try:
            btn = page.locator("button.Modal-closeButton")
            if await btn.count() > 0:
                await btn.click(timeout=2000)
                await page.wait_for_timeout(500)
        except Exception:
            pass

    async def _extract_article(self, page) -> dict:
        """æå–æ–‡ç« ã€‚"""
        # çŸ¥ä¹çš„åçˆ¬æœ‰æ—¶ä¼šè¿”å› JSON é”™è¯¯
        text = await page.locator("body").inner_text()
        if "40362" in text or "è¯·æ±‚å­˜åœ¨å¼‚å¸¸" in text:
            raise Exception("è§¦å‘çŸ¥ä¹åçˆ¬ (40362)")

        # ç­‰å¾…æ ‡é¢˜
        await page.wait_for_selector("h1.Post-Title", timeout=10000)

        title = await page.locator("h1.Post-Title").inner_text()
        author = await self._safe_text(
            page, ".AuthorInfo span.UserLink-Name", "æœªçŸ¥ä½œè€…"
        )
        if author == "æœªçŸ¥ä½œè€…":
            author = await self._safe_text(
                page, ".AuthorInfo-name .UserLink-link", "æœªçŸ¥ä½œè€…"
            )
        
        date = await self._extract_date(page)

        # ä¼˜å…ˆç”±å®¹å™¨æ‰¾
        rich = page.locator(".Post-RichTextContainer .RichText").first
        if await rich.count() > 0:
            html = await rich.inner_html()
        else:
            html = await page.locator(".RichText").first.inner_html()

        # å°è¯•è·å–å¤´å›¾
        try:
            title_img = page.locator("img.TitleImage").first
            if await title_img.count() > 0:
                src = await title_img.get_attribute("src")
                if src:
                    html = f'<img src="{src}" alt="TitleImage"><br>{html}'
        except Exception:
            pass

        return {"title": title.strip(), "author": author.strip(), "html": html, "date": date}

    async def _extract_answer(self, page) -> dict:
        """æå–å›ç­”ã€‚"""
        text = await page.locator("body").inner_text()
        if "40362" in text:
            raise Exception("è§¦å‘çŸ¥ä¹åçˆ¬ (40362)")

        await page.wait_for_selector(".QuestionAnswer-content", timeout=10000)
        
        title = await self._safe_text(page, "h1.QuestionHeader-title", "æœªçŸ¥é—®é¢˜")
        
        # å°è¯•å¤šç§ä½œè€…é€‰æ‹©å™¨
        author = await self._safe_text(page, ".AuthorInfo-name .UserLink-link", "æœªçŸ¥ä½œè€…")
        if author == "æœªçŸ¥ä½œè€…":
            author = await self._safe_text(page, ".AuthorInfo span.UserLink-Name", "æœªçŸ¥ä½œè€…")
        
        date = await self._extract_date(page)
        
        html = await page.locator(".QuestionAnswer-content .RichText").first.inner_html()
        
        return {"title": title.strip(), "author": author.strip(), "html": html, "date": date}

    async def _extract_date(self, page) -> str:
        from datetime import date as dt_date
        try:
            meta = await page.locator('meta[itemprop="datePublished"]').get_attribute("content", timeout=2000)
            if meta: return meta[:10]
        except: pass
        return dt_date.today().isoformat()

    async def _safe_text(self, page, selector: str, default: str) -> str:
        try:
            el = page.locator(selector).first
            return await el.inner_text(timeout=2000)
        except:
            return default

    # â”€â”€ å›¾ç‰‡ä¸‹è½½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @classmethod
    async def download_images(cls, img_urls: list[str], dest: Path) -> dict[str, str]:
        dest.mkdir(parents=True, exist_ok=True)
        url_to_local: dict[str, str] = {}

        # é…ç½®ä»£ç†
        limits = httpx.Limits(max_keepalive_connections=5, max_connections=10)
        async with httpx.AsyncClient(
            headers=cls._IMG_HEADERS,
            timeout=30.0,
            follow_redirects=True,
            proxy=PROXY_SERVER,  # å›¾ç‰‡ä¸‹è½½ä¹Ÿèµ°ä»£ç†
            limits=limits,
        ) as client:
            tasks = [cls._download_one(client, url, dest, url_to_local) for url in img_urls]
            await asyncio.gather(*tasks, return_exceptions=True)

        return url_to_local

    @staticmethod
    async def _download_one(client, url, dest, mapping):
        try:
            if url.startswith("//"): url = "https:" + url
            
            # è¿‡æ»¤ä¸éœ€è¦çš„å›¾ç‰‡
            if "data:image" in url or "equation" in url:
                return

            resp = await client.get(url)
            resp.raise_for_status()

            ext = Path(urlparse(url).path).suffix or ".jpg"
            if len(ext) > 5: ext = ".jpg"
            
            fname = hashlib.md5(url.encode()).hexdigest()[:12] + ext
            fpath = dest / fname

            fpath.write_bytes(resp.content)
            # å¿…é¡»ç”¨ / åˆ†éš”ï¼Œè¦åœ¨ Markdown é‡Œç”¨
            mapping[url] = f"images/{fname}"
        except Exception:
            pass