# 📋 实现计划 (Implementation Plan) - v2.x 升级路线图

> 目标：从“浏览器渲染层归档工具”进化为“纯协议层知乎垂直爬虫及收藏监控系统”。

---

## 阶段一：纯协议层 API 抓包探索 (方向一)
**优先级**：P0  
**目标**：脱离 Playwright 的沉重负载，直接用 Python HttpClient (`httpx` / `requests`) 抓取知乎的内部 JSON 数据。
*   **Step 1**: 在浏览器 F12 网络面板中，找到知乎关于“专栏”或“问题回答”的真实数据 API（包含 `v4/answers` 等）。
*   **Step 2**: 分离请求头生成逻辑（研究现有的 `z_core.js` 或利用第三方开源工具处理 `x-zse-96` 签名）。
*   **Step 3**: 编写 Python 脚本模块，传入 URL 和 `cookies.json`，纯代码请求该 API 并成功获取返回的 JSON 数据结构。
*   **注意事项**：确保代码框架模块化，别人 `git pull` 后只需替换自己的 `cookies.json` 即可直接跑通。

## 阶段二：收藏夹 / 话题增量监控 (方向二)
**优先级**：P1
**目标**：针对你觉得不错的路线，做低频、增量的高价值内容抓取，避免全量爬取导致频繁触发风控/封号。
*   **Step 1**: 解析“知乎收藏夹”或“特定话题”的 API 接口翻页逻辑。
*   **Step 2**: 编写一个“监控任务”脚本。定时比对最新前 1-2 页的数据，发现新加入或新高赞的回答。
*   **Step 3**: 调用【阶段一】的底层接口，只将被识别为“新增内容”的文章或回答抓取并转换为 Markdown。
*   **策略说明**：这个阶段是极度安全的，因为它的访问频率和真人刷知乎完全一样。

## 阶段三：引入 SQLite 轻量化数据存储 (方向三)
**优先级**：P1
**目标**：不再单纯拉取 Markdown 存放在文件夹，而是放入本地数据库进行结构化管理（分类、检索）。
*   **科普**：SQLite 是 Python 系统自带的一种单文件关系型数据库（无需任何环境搭建，直接 `import sqlite3`）。
*   **Step 1**: 设计表结构：如 `articles (id, title, url, author, content_md, collection_name, timestamp)`。
*   **Step 2**: 编写 Python DAL（数据库访问层），将【阶段二】抓取下来的所有数据无缝插入到 SQLite 的 `.db` 本地文件中。
*   **Step 3 (加分项)**: 写一个简单的 Python 脚本用于在终端里快速 `SELECT` 查询你所抓取的数据库。

## 阶段四：代理池与 Cookie 寿命管理池 (方向四)
**优先级**：P3 (当前极低，暂且搁置)
**目标**：处理大规模高并发爬取时的风控对抗。
*   **解释**：当单 IP/单账号被封时，通过代码自动切换其他 IP 甚至购买的代理服务。目前在监控模式下暂时不需要，可以未来再探索。