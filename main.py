"""
main.py â€” çŸ¥ä¹ç¦»çº¿åŒ–å·¥å…·å…¥å£

å…è´£å£°æ˜ï¼š
æœ¬é¡¹ç›®ä»£ç ä»…ç”¨äºå­¦ä¹ ç ”ç©¶ï¼Œä¸¥ç¦ç”¨äºä»»ä½•å•†ä¸šç›®çš„ã€‚
è¯·åœ¨åˆæ³•åˆè§„çš„å‰æä¸‹ä½¿ç”¨ï¼Œå¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•ç”±ä½¿ç”¨æ­¤å·¥å…·å¼•èµ·çš„æ³•å¾‹é£é™©ã€‚
èŒè´£ï¼šç”¨æˆ·äº¤äº’ã€æ–‡ä»¶ç³»ç»Ÿæ“ä½œã€æµæ°´çº¿ä¸²è”ã€‚
"""

import asyncio
import re
import sys
from pathlib import Path

from converter import ZhihuConverter
from scraper import ZhihuDownloader

# ==========================================
# æ‰¹é‡ä¸‹è½½åˆ—è¡¨ (ä¸æƒ³ç”¨å‘½ä»¤è¡Œè¾“å…¥æ—¶ï¼Œåœ¨è¿™é‡Œå¡«å…¥é“¾æ¥)
# æ ¼å¼: ["https://...", "https://..."]
# ==========================================
BATCH_URLS = [
    # "https://zhuanlan.zhihu.com/p/xxxxx",
    # "https://www.zhihu.com/question/xxx/answer/xxx",
]

DATA_DIR = Path(__file__).parent / "data"


# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def sanitize_filename(name: str) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­ macOS / Windows ä¸å…è®¸çš„å­—ç¬¦ã€‚"""
    name = re.sub(r'[/\\:*?"<>|\x00-\x1f]', "_", name)
    name = name.strip(" .")
    if len(name) > 100:
        name = name[:100].rstrip(" .")
    return name or "untitled"


def extract_urls(text: str) -> list[str]:
    """ä»ä»»æ„æ–‡æœ¬ä¸­æå–çŸ¥ä¹é“¾æ¥ã€‚"""
    pattern = r"https?://(?:www\.|zhuanlan\.)?zhihu\.com/(?:p/\d+|question/\d+/answer/\d+)"
    return list(dict.fromkeys(re.findall(pattern, text)))


# â”€â”€ æµæ°´çº¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Pipeline:
    """å•ç¯‡æ–‡ç« çš„å¤„ç†æµæ°´çº¿ï¼šæŠ“å– â†’ ä¸‹è½½å›¾ç‰‡ â†’ è½¬æ¢ â†’ ä¿å­˜ã€‚"""

    def __init__(self, url: str, output_dir: Path = DATA_DIR):
        self.url = url
        self.output_dir = output_dir

    async def run(self) -> Path:
        """æ‰§è¡Œå®Œæ•´æµç¨‹ï¼Œè¿”å›è¾“å‡ºç›®å½•è·¯å¾„ã€‚"""
        # 1. æŠ“å–é¡µé¢
        info = await ZhihuDownloader(self.url).fetch_page()
        title = info["title"]
        author = info["author"]
        date = info["date"]
        html = info["html"]

        print(f"ğŸ“„ æ ‡é¢˜: {title}")
        print(f"âœï¸  ä½œè€…: {author}")

        # 2. å‡†å¤‡è¾“å‡ºç›®å½•
        folder_name = sanitize_filename(f"[{date}] {title} - {author}")
        folder = self.output_dir / folder_name
        img_dir = folder / "images"
        folder.mkdir(parents=True, exist_ok=True)

        # 3. æå–å›¾ç‰‡ URL å¹¶ä¸‹è½½
        img_urls = ZhihuConverter.extract_image_urls(html)
        print(f"ğŸ–¼ï¸  å‘ç° {len(img_urls)} å¼ å›¾ç‰‡ï¼Œæ­£åœ¨ä¸‹è½½...")
        img_map = await ZhihuDownloader.download_images(img_urls, img_dir)
        print(f"âœ… æˆåŠŸä¸‹è½½ {len(img_map)} å¼ å›¾ç‰‡")

        # 4. HTML â†’ Markdown
        converter = ZhihuConverter(img_map=img_map)
        md = converter.convert(html)

        # 5. æ‹¼æ¥å…ƒä¿¡æ¯å¤´ + ä¿å­˜
        header = (
            f"# {title}\n\n"
            f"> **ä½œè€…**: {author}  \n"
            f"> **æ¥æº**: [{self.url}]({self.url})  \n"
            f"> **æ—¥æœŸ**: {date}\n\n"
            f"---\n\n"
        )
        (folder / "index.md").write_text(header + md, encoding="utf-8")
        print(f"ğŸ’¾ å·²ä¿å­˜è‡³: {folder}")

        # æ¸…ç†ç©ºå›¾ç‰‡ç›®å½•
        if img_dir.exists() and not any(img_dir.iterdir()):
            img_dir.rmdir()

        return folder


# â”€â”€ ä¸»å¾ªç¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main() -> None:
    """æŒç»­æ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œé€ä¸ªå¤„ç†é“¾æ¥ã€‚"""
    print("=" * 60)
    print("ğŸ“š çŸ¥ä¹ç¦»çº¿åŒ–å·¥å…·")
    print("=" * 60)
    print("æ”¯æŒé“¾æ¥ç±»å‹:")
    print("  - ä¸“æ æ–‡ç« : https://zhuanlan.zhihu.com/p/xxxxxxx")
    print("  - é—®é¢˜å›ç­”: https://www.zhihu.com/question/xxx/answer/xxx")
    print("è¾“å…¥ q é€€å‡º\n")

    while True:
        # â”€â”€ è·å–å¾…å¤„ç†é“¾æ¥ â”€â”€
        if BATCH_URLS:
            print(f"ğŸ“‹ æ£€æµ‹åˆ° BATCH_URLS ä¸­æœ‰ {len(BATCH_URLS)} ä¸ªé“¾æ¥ï¼Œå¼€å§‹è‡ªåŠ¨å¤„ç†...")
            target_urls = list(BATCH_URLS)
            BATCH_URLS.clear()
        else:
            try:
                print("\nğŸ”— è¯·ç²˜è´´çŸ¥ä¹é“¾æ¥ (å¯åŒ…å«å…¶å®ƒæ–‡å­—): ", end="", flush=True)
                user_input = sys.stdin.readline().strip()
            except (EOFError, KeyboardInterrupt):
                print("\nğŸ‘‹ å†è§!")
                break

            if user_input.lower() == "q":
                print("ğŸ‘‹ å†è§!")
                break

            target_urls = extract_urls(user_input)

        if not target_urls:
            print("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆé“¾æ¥ï¼Œè¯·é‡æ–°è¾“å…¥")
            continue

        # â”€â”€ é€ä¸ªå¤„ç† â”€â”€
        print(f"ğŸ” æ£€æµ‹åˆ° {len(target_urls)} ä¸ªé“¾æ¥")

        for url in target_urls:
            print(f"\n{'='*60}")
            print(f"ğŸ“¥ æ­£åœ¨æŠ“å–: {url}")
            try:
                await Pipeline(url).run()
            except Exception as e:
                err_msg = str(e)
                if "ERR_PROXY_CONNECTION_FAILED" in err_msg or "Connection refused" in err_msg:
                    print(f"\nâŒ ä»£ç†è¿æ¥å¤±è´¥: {e}")
                    print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥æœ¬åœ°ä»£ç†æ˜¯å¦å¼€å¯ï¼Œæˆ–åœ¨ scraper.py ä¸­å°† PROXY_SERVER è®¾ä¸º Noneã€‚")
                else:
                    print(f"âŒ å¤„ç†å¤±è´¥ [{url}]: {e}")
                print("ğŸ”„ è·³è¿‡å½“å‰é“¾æ¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...")

        print(f"\nâœ¨ æœ¬æ‰¹æ¬¡å¤„ç†å®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨ {DATA_DIR.resolve()}")


if __name__ == "__main__":
    asyncio.run(main())