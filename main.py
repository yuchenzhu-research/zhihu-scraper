"""
main.py â€” çŸ¥ä¹ç¦»çº¿åŒ–å·¥å…·å…¥å£
æ”¯æŒç”¨æˆ·ç²˜è´´å« URL çš„æ‚ä¹±æ–‡æœ¬ï¼Œè‡ªåŠ¨æå–å¹¶é€ä¸ªå¤„ç†ã€‚
"""

import asyncio
import re
import sys
from pathlib import Path

from converter import get_image_urls, html_to_markdown, sanitize_filename
from scraper import ZhihuDownloader

# ==========================================
# æ‰¹é‡ä¸‹è½½åˆ—è¡¨ (å¦‚æœä½ ä¸æƒ³ä½¿ç”¨å‘½ä»¤è¡Œè¾“å…¥ï¼Œå¯ä»¥åœ¨è¿™é‡Œå¡«å…¥é“¾æ¥)
# æ ¼å¼: ["https://...", "https://..."]
# ==========================================
BATCH_URLS = [
    # "https://zhuanlan.zhihu.com/p/xxxxx",
    # "https://www.zhihu.com/question/xxx/answer/xxx",
]

DATA_DIR = Path(__file__).parent / "data"


def extract_urls(text: str) -> list[str]:
    """ä»ä»»æ„æ–‡æœ¬ä¸­æå–çŸ¥ä¹é“¾æ¥ã€‚"""
    pattern = r"https?://(?:zhuanlan\.zhihu\.com/p/\d+|www\.zhihu\.com/question/\d+/answer/\d+)"
    return list(dict.fromkeys(re.findall(pattern, text)))


async def process_one(url: str) -> None:
    """å¤„ç†å•ä¸ªçŸ¥ä¹é“¾æ¥ï¼šæŠ“å– â†’ ä¸‹è½½å›¾ç‰‡ â†’ è½¬æ¢ â†’ ä¿å­˜ã€‚"""
    print(f"\n{'='*60}")
    print(f"ğŸ“¥ æ­£åœ¨æŠ“å–: {url}")

    downloader = ZhihuDownloader(url)

    # 1. æŠ“å–é¡µé¢
    info = await downloader.fetch_page()
    title = info["title"]
    author = info["author"]
    date = info["date"]
    html = info["html"]

    print(f"ğŸ“„ æ ‡é¢˜: {title}")
    print(f"âœï¸  ä½œè€…: {author}")

    # 2. æ„å»ºè¾“å‡ºç›®å½•
    folder_name = sanitize_filename(f"[{date}] {title} - {author}")
    out_dir = DATA_DIR / folder_name
    img_dir = out_dir / "images"
    out_dir.mkdir(parents=True, exist_ok=True)

    # 3. æå–å¹¶ä¸‹è½½å›¾ç‰‡
    img_urls = get_image_urls(html)
    print(f"ğŸ–¼ï¸  å‘ç° {len(img_urls)} å¼ å›¾ç‰‡ï¼Œæ­£åœ¨ä¸‹è½½...")

    img_map = await ZhihuDownloader.download_images(img_urls, img_dir)
    print(f"âœ… æˆåŠŸä¸‹è½½ {len(img_map)} å¼ å›¾ç‰‡")

    # 4. è½¬æ¢ Markdown
    md_content = html_to_markdown(html, img_map)

    # åŠ ä¸Šå…ƒä¿¡æ¯å¤´
    header = (
        f"# {title}\n\n"
        f"> **ä½œè€…**: {author}  \n"
        f"> **æ¥æº**: [{url}]({url})  \n"
        f"> **æ—¥æœŸ**: {date}\n\n"
        f"---\n\n"
    )
    md_content = header + md_content

    # 5. ä¿å­˜
    md_path = out_dir / "index.md"
    md_path.write_text(md_content, encoding="utf-8")
    print(f"ğŸ’¾ å·²ä¿å­˜è‡³: {out_dir}")

    # å¦‚æœæ²¡æœ‰å›¾ç‰‡å°±åˆ é™¤ç©ºç›®å½•
    if img_dir.exists() and not any(img_dir.iterdir()):
        img_dir.rmdir()


async def main() -> None:
    """ä¸»å¾ªç¯ï¼šæŒç»­æ¥æ”¶ç”¨æˆ·è¾“å…¥ã€‚"""
    print("=" * 60)
    print("ğŸ“š çŸ¥ä¹ç¦»çº¿åŒ–å·¥å…·")
    print("=" * 60)
    print("æ”¯æŒé“¾æ¥ç±»å‹:")
    print("  - ä¸“æ æ–‡ç« : https://zhuanlan.zhihu.com/p/xxxxxxx")
    print("  - é—®é¢˜å›ç­”: https://www.zhihu.com/question/xxx/answer/xxx")
    print("è¾“å…¥ q é€€å‡º\n")

    while True:
        # ä¼˜å…ˆå¤„ç† BATCH_URLS
        if BATCH_URLS:
            print(f"ğŸ“‹ æ£€æµ‹åˆ° BATCH_URLS ä¸­æœ‰ {len(BATCH_URLS)} ä¸ªé“¾æ¥ï¼Œå¼€å§‹è‡ªåŠ¨å¤„ç†...")
            target_urls = list(BATCH_URLS)
            # å¤„ç†å®Œæ‰¹æ¬¡é“¾æ¥åæ¸…ç©ºï¼Œé¿å…é‡å¤å¤„ç†ï¼Œå¹¶é€€å‡ºå¾ªç¯
            BATCH_URLS.clear()
        else:
            try:
                print(f"\nğŸ”— è¯·ç²˜è´´çŸ¥ä¹é“¾æ¥ (å¯åŒ…å«å…¶å®ƒæ–‡å­—): ", end="", flush=True)
                user_input = sys.stdin.readline().strip()
            except (EOFError, KeyboardInterrupt):
                print("\nğŸ‘‹ å†è§!")
                break

            if user_input.lower() == "q":
                print("ğŸ‘‹ å†è§!")
                break

            target_urls = re.findall(
                r"https?://(?:www\.|zhuanlan\.)?zhihu\.com/(?:p/\d+|question/\d+/answer/\d+)",
                user_input,
            )

        if not target_urls:
            print("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆé“¾æ¥ï¼Œè¯·é‡æ–°è¾“å…¥")
            if not BATCH_URLS: # Only continue if not in batch mode
                continue
            else: # If BATCH_URLS was empty, break
                break

        print(f"ğŸ” æ£€æµ‹åˆ° {len(target_urls)} ä¸ªé“¾æ¥")

        for url in target_urls:
            try:
                await process_one(url)
            except Exception as e:
                err_msg = str(e)
                if "ERR_PROXY_CONNECTION_FAILED" in err_msg or "Connection refused" in err_msg:
                    print(f"\nâŒ ä»£ç†è¿æ¥å¤±è´¥: {e}")
                    print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥æœ¬åœ°ä»£ç† (127.0.0.1:1082) æ˜¯å¦å¼€å¯ã€‚")
                    print("   æˆ–è€…åœ¨ scraper.py ä¸­å°† PROXY_SERVER è®¾ç½®ä¸º Noneã€‚")
                else:
                    print(f"âŒ å¤„ç†å¤±è´¥ [{url}]: {e}")
                
                # æ‰¹é‡å¤„ç†æ—¶ä¸å› ä¸ºå•ä¸ªå¤±è´¥è€Œä¸­æ–­
                if BATCH_URLS:
                    print("ğŸ”„ è·³è¿‡å½“å‰é“¾æ¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...")
                    continue

        print(f"\nâœ¨ æœ¬æ‰¹æ¬¡å¤„ç†å®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨ {DATA_DIR.resolve()}")


if __name__ == "__main__":
    asyncio.run(main())