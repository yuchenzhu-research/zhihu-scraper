"""
main.py â€” çŸ¥ä¹ç¦»çº¿åŒ–å·¥å…·å…¥å£

å…è´£å£°æ˜ï¼š
æœ¬é¡¹ç›®ä»£ç ä»…ç”¨äºå­¦ä¹ ç ”ç©¶ï¼Œä¸¥ç¦ç”¨äºä»»ä½•å•†ä¸šç›®çš„ã€‚
è¯·åœ¨åˆæ³•åˆè§„çš„å‰æä¸‹ä½¿ç”¨ï¼Œå¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•ç”±ä½¿ç”¨æ­¤å·¥å…·å¼•èµ·çš„æ³•å¾‹é£é™©ã€‚
èŒè´£ï¼šç”¨æˆ·äº¤äº’ã€æ–‡ä»¶ç³»ç»Ÿæ“ä½œã€æµæ°´çº¿ä¸²è”ã€‚
"""

import asyncio
import re
import sys
from datetime import datetime
from pathlib import Path

from typing import Optional
from core.converter import ZhihuConverter
from core.scraper import ZhihuDownloader

# ==========================================
# æ‰¹é‡ä¸‹è½½åˆ—è¡¨ (ä¸æƒ³ç”¨å‘½ä»¤è¡Œè¾“å…¥æ—¶ï¼Œåœ¨è¿™é‡Œå¡«å…¥é“¾æ¥)
# æ ¼å¼: ["https://...", "https://..."]
# ==========================================
BATCH_URLS = [
    # "https://zhuanlan.zhihu.com/p/xxxxx",
    # "https://www.zhihu.com/question/xxx/answer/xxx",
]

DATA_DIR = Path(__file__).parent / "data"


# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def sanitize_filename(name: str) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­ macOS / Windows ä¸å…è®¸çš„å­—ç¬¦ã€‚"""
    name = re.sub(r'[/\\:*?"<>|\x00-\x1f]', "_", name)
    name = name.strip(" .")
    if len(name) > 100:
        name = name[:100].rstrip(" .")
    return name or "untitled"


def extract_urls(text: str) -> list[str]:
    """ä»ä»»æ„æ–‡æœ¬ä¸­æå–çŸ¥ä¹é“¾æ¥ã€‚"""
    pattern = r"https?://(?:www\.|zhuanlan\.)?zhihu\.com/(?:p/\d+|question/\d+(?:/answer/\d+)?)"
    return list(dict.fromkeys(re.findall(pattern, text)))

def parse_question_options(user_input: str) -> dict:
    """
    è§£æç”¨æˆ·å¯¹é—®é¢˜æŠ“å–çš„é€‰é¡¹ã€‚
    - ç©ºæˆ–å›è½¦: é»˜è®¤æŠ“å–å‰ 3 ä¸ª (ç¨³å®š)
    - æ•°å­— N: æŠ“å–å‰ N ä¸ª (è‡ªå®šä¹‰)
    """
    user_input = user_input.lower().strip()
    
    # -------------------------------------------------------------
    # æ¨¡å¼ A: æ¸¸å®¢æ¨¡å¼ (æ—  Cookie) -> å¼ºåˆ¶é»˜è®¤ Top 3
    # -------------------------------------------------------------
    if not ZhihuDownloader(url="").has_valid_cookies():
        return {"start": 0, "limit": 3}

    # -------------------------------------------------------------
    # æ¨¡å¼ B: ç™»å½•æ¨¡å¼ (æœ‰ Cookie) -> æ˜¾ç¤ºå­èœå•
    # -------------------------------------------------------------
    print("\n   [1] æŒ‰æ•°é‡: æŠ“å–å‰ N ä¸ªå›ç­”")
    print("   [2] æŒ‰èŒƒå›´: æŒ‡å®šèµ·æ­¢ç­”ä¸»/é“¾æ¥ (é—­åŒºé—´)")
    print("ğŸ‘‰ è¯·è¾“å…¥ (1/2): ", end="", flush=True)
    
    mode_choice = sys.stdin.readline().strip()
    
    # --- æ¨¡å¼ 2.1: æŒ‰æ•°é‡ ---
    if mode_choice == "1":
        print("ğŸ‘‰ è¯·è¾“å…¥è¦æŠ“å–çš„æ•°é‡: ", end="", flush=True)
        try:
            limit = int(sys.stdin.readline().strip())
            return {"start": 0, "limit": limit}
        except:
            print("âš ï¸è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ 20")
            return {"start": 0, "limit": 20}
            
    # --- æ¨¡å¼ 2.2: æŒ‰èŒƒå›´ ---
    elif mode_choice == "2":
        print("ğŸ‘‰ è¯·è¾“å…¥èµ·å§‹ (ç­”ä¸»åå­—æˆ–å›ç­”é“¾æ¥): ", end="", flush=True)
        start_input = sys.stdin.readline().strip()
        print("ğŸ‘‰ è¯·è¾“å…¥ç»“æŸ (ç­”ä¸»åå­—æˆ–å›ç­”é“¾æ¥): ", end="", flush=True)
        end_input = sys.stdin.readline().strip()
        
        start_anchor = _parse_anchor(start_input)
        end_anchor = _parse_anchor(end_input)
        
        if not start_anchor or not end_anchor:
            print("âš ï¸ è¾“å…¥æ— æ•ˆï¼Œå›é€€åˆ°é»˜è®¤ Top 3")
            return {"start": 0, "limit": 3}
            
        return {
            "start": 0, 
            "limit": 3, # å ä½
            "start_anchor": start_anchor,
            "end_anchor": end_anchor
        }
    
    # --- é»˜è®¤ fallback ---
    else:
        # å…¼å®¹æ—§ä¹ æƒ¯ï¼šå¦‚æœç›´æ¥è¾“æ•°å­—ï¼Œå½“åšæ¨¡å¼ 2.1
        try:
            limit = int(mode_choice)
            print(f"ğŸ’¡ è¯†åˆ«ä¸ºæŠ“å–å‰ {limit} ä¸ª")
            return {"start": 0, "limit": limit}
        except:
            print("âš ï¸ é€‰é¡¹æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤ Top 3")
            return {"start": 0, "limit": 3}

def _parse_anchor(val: str) -> Optional[dict]:
    """è§£æé”šç‚¹è¾“å…¥: é“¾æ¥ -> answer_id, åå­— -> author"""
    if not val: return None
    
    # å°è¯•æå– answer id
    # https://www.zhihu.com/question/xxx/answer/12345
    m = re.search(r"answer/(\d+)", val)
    if m:
        return {"type": "answer_id", "value": m.group(1)}
    
    # çº¯æ•°å­—ä¹Ÿå½“åš answer id? ä¸ï¼Œåå­—å¯èƒ½æ˜¯çº¯æ•°å­—ã€‚
    # é»˜è®¤å½“åšåå­—
    return {"type": "author", "value": val}


# â”€â”€ æµæ°´çº¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Pipeline:
    """å•ç¯‡æ–‡ç« çš„å¤„ç†æµæ°´çº¿ï¼šæŠ“å– â†’ ä¸‹è½½å›¾ç‰‡ â†’ è½¬æ¢ â†’ ä¿å­˜ã€‚"""

    def __init__(self, url: str, output_dir: Path = DATA_DIR, scrape_config: Optional[dict] = None):
        self.url = url
        self.output_dir = output_dir
        self.scrape_config = scrape_config or {}

    async def run(self) -> None:
        """æ‰§è¡Œå®Œæ•´æµç¨‹ï¼Œæ”¯æŒå•ä¸ªæˆ–å¤šä¸ªç»“æœã€‚"""
        downloader = ZhihuDownloader(self.url)
        # ä¼ é€’é…ç½®ç»™ fetch_page
        data = await downloader.fetch_page(**self.scrape_config)

        if isinstance(data, list):
            print(f"ğŸ“¦ æŠ“å–åˆ° {len(data)} ä¸ªå†…å®¹ï¼Œå¼€å§‹å¤„ç†...")
            for i, item in enumerate(data):
                print(f"  > ({i+1}/{len(data)}) å¤„ç†: {item.get('author', 'Unknown')}")
                await self._process_one(item, downloader.page_type)
        else:
            await self._process_one(data, downloader.page_type)

    async def _process_one(self, info: dict, page_type: str) -> Path:
        title = info["title"]
        author = info["author"]
        date = info["date"]
        html = info["html"]

        today = datetime.now().strftime("%Y-%m-%d")
        safe_title = sanitize_filename(title)
        safe_author = sanitize_filename(author)

        if page_type == "question":
            # data/[Date] QuestionTitle / Author.md
            folder_name = sanitize_filename(f"[{today}] {title}")
            file_name = f"{safe_author}.md"
        else:
            # data/[Date] Title - Author / index.md
            folder_name = sanitize_filename(f"[{date}] {title} - {author}")
            file_name = "index.md"

        folder = self.output_dir / folder_name
        img_dir = folder / "images"
        folder.mkdir(parents=True, exist_ok=True)

        # 3. æå–å›¾ç‰‡ URL å¹¶ä¸‹è½½
        img_urls = ZhihuConverter.extract_image_urls(html)
        if img_urls:
            print(f"ğŸ–¼ï¸  å‘ç° {len(img_urls)} å¼ å›¾ç‰‡ï¼Œæ­£åœ¨ä¸‹è½½...")
            img_map = await ZhihuDownloader.download_images(img_urls, img_dir)
            print(f"âœ… æˆåŠŸä¸‹è½½ {len(img_map)} å¼ å›¾ç‰‡")
        else:
            img_map = {}

        # 4. HTML â†’ Markdown
        converter = ZhihuConverter(img_map=img_map)
        md = converter.convert(html)

        # 5. æ‹¼æ¥å…ƒä¿¡æ¯å¤´ + ä¿å­˜
        header = (
            f"# {title}\n\n"
            f"> **ä½œè€…**: {author}  \n"
            f"> **æ¥æº**: [{self.url}]({self.url})  \n"
            f"> **æ—¥æœŸ**: {date}\n\n"
            f"---\n\n"
        )

        (folder / file_name).write_text(header + md, encoding="utf-8")
        print(f"ğŸ’¾ å·²ä¿å­˜è‡³: {folder / file_name}")

        # æ¸…ç†ç©ºå›¾ç‰‡ç›®å½•
        if img_dir.exists() and not any(img_dir.iterdir()):
            img_dir.rmdir()

        return folder


# â”€â”€ ä¸»å¾ªç¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main() -> None:
    """æŒç»­æ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œé€ä¸ªå¤„ç†é“¾æ¥ã€‚"""
    print("=" * 60)
    print("ğŸ“š çŸ¥ä¹ç¦»çº¿åŒ–å·¥å…·")
    print("=" * 60)
    print("æ”¯æŒé“¾æ¥ç±»å‹:")
    print("  - ä¸“æ æ–‡ç« : https://zhuanlan.zhihu.com/p/xxxxxxx")
    print("  - é—®é¢˜å›ç­”: https://www.zhihu.com/question/xxx/answer/xxx")
    print("  - å®Œæ•´é—®é¢˜: https://www.zhihu.com/question/xxx")
    print("\nğŸ’¡ æç¤º: é»˜è®¤æŠ“å– 3 æ¡æœ€ç¨³å®šã€‚å¦‚éœ€å¤§é‡æŠ“å–ï¼Œè¯·åœ¨ cookies.json ä¸­å¡«å…¥ z_c0")
    print("è¾“å…¥ q é€€å‡º\n")

    should_prompt = True
    while True:
        # â”€â”€ è·å–å¾…å¤„ç†é“¾æ¥ â”€â”€
        if BATCH_URLS:
            print(f"ğŸ“‹ æ£€æµ‹åˆ° BATCH_URLS ä¸­æœ‰ {len(BATCH_URLS)} ä¸ªé“¾æ¥ï¼Œå¼€å§‹è‡ªåŠ¨å¤„ç†...")
            target_urls = list(BATCH_URLS)
            BATCH_URLS.clear()
            should_prompt = True
        else:
            try:
                if should_prompt:
                    print("\nğŸ”— è¯·ç²˜è´´çŸ¥ä¹é“¾æ¥ (å¯åŒ…å«å…¶å®ƒæ–‡å­—): ", end="", flush=True)
                    should_prompt = False

                user_input = sys.stdin.readline().strip()
            except (EOFError, KeyboardInterrupt):
                print("\nğŸ‘‹ å†è§!")
                break

            if user_input.lower() == "q":
                print("ğŸ‘‹ å†è§!")
                break

            target_urls = extract_urls(user_input)

        if not target_urls:
            # åªæœ‰å½“ç”¨æˆ·è¾“å…¥ä¸ºç©ºæ—¶æ‰æç¤ºï¼Œé¿å…å› ä¸ºå¤åˆ¶äº†æ ‡é¢˜è¡Œå¯¼è‡´æŠ¥é”™åˆ·å±
            if not user_input:
                 should_prompt = True
            continue

        # â”€â”€ é€ä¸ªå¤„ç† â”€â”€
        print(f"ğŸ” æ£€æµ‹åˆ° {len(target_urls)} ä¸ªé“¾æ¥")

        for url in target_urls:
            print(f"\n{'='*60}")
            
            # æ£€æµ‹æ˜¯å¦ä¸ºé—®é¢˜é“¾æ¥ï¼Œå¦‚æœæ˜¯ï¼Œè¯¢é—®æŠ“å–èŒƒå›´
            scrape_config = {}
            if "/question/" in url and "/answer/" not in url:
                try:
                    print(f"âš™ï¸  æ£€æµ‹åˆ°é—®é¢˜é“¾æ¥: {url}")
                    print(f"âš™ï¸  æ£€æµ‹åˆ°é—®é¢˜é“¾æ¥: {url}")
                    
                    is_login = ZhihuDownloader(url).has_valid_cookies()
                    status = "âœ… å·²ç™»å½• (è§£é”å…¨éƒ¨åŠŸèƒ½)" if is_login else "âŒ æ¸¸å®¢æ¨¡å¼ (ä»…é™å‰ 3 æ¡)"
                    print(f"   ğŸª Cookie çŠ¶æ€: {status}")
                    
                    if not is_login:
                        print("   (æŒ‰ Enter ç»§ç»­æŠ“å–å‰ 3 æ¡)")
                        sys.stdin.readline()
                        scrape_config = {"start": 0, "limit": 3}
                    else:
                        scrape_config = parse_question_options("") # ä¼ å…¥ç©ºä¸²è§¦å‘å†…éƒ¨äº¤äº’
                        
                    if "start_anchor" in scrape_config:
                        s = scrape_config['start_anchor']['value']
                        e = scrape_config['end_anchor']['value']
                        print(f"âœ… å·²è®¾å®š: æŠ“å–èŒƒå›´ {s} -> {e}")
                    else:
                        print(f"âœ… å·²è®¾å®š: æŠ“å–å‰ {scrape_config['limit']} ä¸ªå›ç­”")
                except (KeyboardInterrupt, EOFError):
                    print("\nğŸ›‘ å–æ¶ˆæ“ä½œ")
                    continue

            print(f"ğŸ“¥ æ­£åœ¨æŠ“å–: {url}")
            try:
                await Pipeline(url, scrape_config=scrape_config).run()
            except Exception as e:
                err_msg = str(e)
                if "ERR_PROXY_CONNECTION_FAILED" in err_msg or "Connection refused" in err_msg:
                    print(f"\nâŒ ä»£ç†è¿æ¥å¤±è´¥: {e}")
                    print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥æœ¬åœ°ä»£ç†æ˜¯å¦å¼€å¯ï¼Œæˆ–åœ¨ scraper.py ä¸­å°† PROXY_SERVER è®¾ä¸º Noneã€‚")
                else:
                    print(f"âŒ å¤„ç†å¤±è´¥ [{url}]: {e}")
                print("ğŸ”„ è·³è¿‡å½“å‰é“¾æ¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...")

        print(f"\nâœ¨ æœ¬æ‰¹æ¬¡å¤„ç†å®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨ {DATA_DIR.resolve()}")
        should_prompt = True


if __name__ == "__main__":
    asyncio.run(main())