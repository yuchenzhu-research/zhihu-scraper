"""
main.py â€” çŸ¥ä¹ç¦»çº¿åŒ–å·¥å…·å…¥å£

å…è´£å£°æ˜ï¼š
æœ¬é¡¹ç›®ä»£ç ä»…ç”¨äºå­¦ä¹ ç ”ç©¶ï¼Œä¸¥ç¦ç”¨äºä»»ä½•å•†ä¸šç›®çš„ã€‚
è¯·åœ¨åˆæ³•åˆè§„çš„å‰æä¸‹ä½¿ç”¨ï¼Œå¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•ç”±ä½¿ç”¨æ­¤å·¥å…·å¼•èµ·çš„æ³•å¾‹é£é™©ã€‚
èŒè´£ï¼šç”¨æˆ·äº¤äº’ã€æ–‡ä»¶ç³»ç»Ÿæ“ä½œã€æµæ°´çº¿ä¸²è”ã€‚
"""

import asyncio
import re
import sys
from datetime import datetime
from pathlib import Path

from converter import ZhihuConverter
from scraper import ZhihuDownloader

# ==========================================
# æ‰¹é‡ä¸‹è½½åˆ—è¡¨ (ä¸æƒ³ç”¨å‘½ä»¤è¡Œè¾“å…¥æ—¶ï¼Œåœ¨è¿™é‡Œå¡«å…¥é“¾æ¥)
# æ ¼å¼: ["https://...", "https://..."]
# ==========================================
BATCH_URLS = [
    # "https://zhuanlan.zhihu.com/p/xxxxx",
    # "https://www.zhihu.com/question/xxx/answer/xxx",
]

DATA_DIR = Path(__file__).parent / "data"


# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def sanitize_filename(name: str) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­ macOS / Windows ä¸å…è®¸çš„å­—ç¬¦ã€‚"""
    name = re.sub(r'[/\\:*?"<>|\x00-\x1f]', "_", name)
    name = name.strip(" .")
    if len(name) > 100:
        name = name[:100].rstrip(" .")
    return name or "untitled"


def extract_urls(text: str) -> list[str]:
    """ä»ä»»æ„æ–‡æœ¬ä¸­æå–çŸ¥ä¹é“¾æ¥ã€‚"""
    pattern = r"https?://(?:www\.|zhuanlan\.)?zhihu\.com/(?:p/\d+|question/\d+(?:/answer/\d+)?)"
    return list(dict.fromkeys(re.findall(pattern, text)))

def parse_question_options(user_input: str) -> dict:
    """è§£æç”¨æˆ·å¯¹é—®é¢˜æŠ“å–çš„é€‰é¡¹ (Top N æˆ– Range æˆ– æ™ºèƒ½æ¨¡å¼)ã€‚"""
    user_input = user_input.lower().strip()
    
    # 1. é»˜è®¤
    if not user_input:
        return {"start": 0, "limit": 20}
    
    # 2. æ™ºèƒ½æ¨¡å¼ (Smart Stop)
    if user_input == "s":
        return {"start": 0, "limit": 10, "smart_stop": True}
    
    # 3. Range: 10-20
    if "-" in user_input:
        try:
            parts = user_input.split("-")
            start = max(0, int(parts[0]) - 1)  # è½¬ä¸º 0-indexed
            end = int(parts[1])
            return {"start": start, "limit": max(1, end - start)}
        except:
            pass
            
    # 4. Top N: 50
    try:
        limit = int(user_input)
        return {"start": 0, "limit": limit}
    except:
        pass
        
    print("âš ï¸  è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½® (Top 20)")
    return {"start": 0, "limit": 20}


# â”€â”€ æµæ°´çº¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Pipeline:
    """å•ç¯‡æ–‡ç« çš„å¤„ç†æµæ°´çº¿ï¼šæŠ“å– â†’ ä¸‹è½½å›¾ç‰‡ â†’ è½¬æ¢ â†’ ä¿å­˜ã€‚"""

    def __init__(self, url: str, output_dir: Path = DATA_DIR, scrape_config: dict = None):
        self.url = url
        self.output_dir = output_dir
        self.scrape_config = scrape_config or {}

    async def run(self) -> None:
        """æ‰§è¡Œå®Œæ•´æµç¨‹ï¼Œæ”¯æŒå•ä¸ªæˆ–å¤šä¸ªç»“æœã€‚"""
        downloader = ZhihuDownloader(self.url)
        # ä¼ é€’é…ç½®ç»™ fetch_page
        data = await downloader.fetch_page(**self.scrape_config)

        if isinstance(data, list):
            print(f"ğŸ“¦ æŠ“å–åˆ° {len(data)} ä¸ªå†…å®¹ï¼Œå¼€å§‹å¤„ç†...")
            for i, item in enumerate(data):
                print(f"  > ({i+1}/{len(data)}) å¤„ç†: {item.get('author', 'Unknown')}")
                await self._process_one(item, downloader.page_type)
        else:
            await self._process_one(data, downloader.page_type)

    async def _process_one(self, info: dict, page_type: str) -> Path:
        title = info["title"]
        author = info["author"]
        date = info["date"]
        html = info["html"]

        today = datetime.now().strftime("%Y-%m-%d")
        safe_title = sanitize_filename(title)
        safe_author = sanitize_filename(author)

        if page_type == "question":
            # data/[Date] QuestionTitle / Author.md
            folder_name = sanitize_filename(f"[{today}] {title}")
            file_name = f"{safe_author}.md"
        else:
            # data/[Date] Title - Author / index.md
            folder_name = sanitize_filename(f"[{date}] {title} - {author}")
            file_name = "index.md"

        folder = self.output_dir / folder_name
        img_dir = folder / "images"
        folder.mkdir(parents=True, exist_ok=True)

        # 3. æå–å›¾ç‰‡ URL å¹¶ä¸‹è½½
        img_urls = ZhihuConverter.extract_image_urls(html)
        if img_urls:
            print(f"ğŸ–¼ï¸  å‘ç° {len(img_urls)} å¼ å›¾ç‰‡ï¼Œæ­£åœ¨ä¸‹è½½...")
            img_map = await ZhihuDownloader.download_images(img_urls, img_dir)
            print(f"âœ… æˆåŠŸä¸‹è½½ {len(img_map)} å¼ å›¾ç‰‡")
        else:
            img_map = {}

        # 4. HTML â†’ Markdown
        converter = ZhihuConverter(img_map=img_map)
        md = converter.convert(html)

        # 5. æ‹¼æ¥å…ƒä¿¡æ¯å¤´ + ä¿å­˜
        header = (
            f"# {title}\n\n"
            f"> **ä½œè€…**: {author}  \n"
            f"> **æ¥æº**: [{self.url}]({self.url})  \n"
            f"> **æ—¥æœŸ**: {date}\n\n"
            f"---\n\n"
        )

        (folder / file_name).write_text(header + md, encoding="utf-8")
        print(f"ğŸ’¾ å·²ä¿å­˜è‡³: {folder / file_name}")

        # æ¸…ç†ç©ºå›¾ç‰‡ç›®å½•
        if img_dir.exists() and not any(img_dir.iterdir()):
            img_dir.rmdir()

        return folder


# â”€â”€ ä¸»å¾ªç¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main() -> None:
    """æŒç»­æ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œé€ä¸ªå¤„ç†é“¾æ¥ã€‚"""
    print("=" * 60)
    print("ğŸ“š çŸ¥ä¹ç¦»çº¿åŒ–å·¥å…·")
    print("=" * 60)
    print("æ”¯æŒé“¾æ¥ç±»å‹:")
    print("  - ä¸“æ æ–‡ç« : https://zhuanlan.zhihu.com/p/xxxxxxx")
    print("  - é—®é¢˜å›ç­”: https://www.zhihu.com/question/xxx/answer/xxx")
    print("  - å®Œæ•´é—®é¢˜: https://www.zhihu.com/question/xxx")
    print("\nğŸ’¡ æç¤º: å¦‚æœæŠ“å–å›ç­”ä¸å…¨ï¼Œè¯·åœ¨ cookies.json ä¸­å¡«å…¥ Cookie (å°¤å…¶æ˜¯ z_c0)")
    print("è¾“å…¥ q é€€å‡º\n")

    should_prompt = True
    while True:
        # â”€â”€ è·å–å¾…å¤„ç†é“¾æ¥ â”€â”€
        if BATCH_URLS:
            print(f"ğŸ“‹ æ£€æµ‹åˆ° BATCH_URLS ä¸­æœ‰ {len(BATCH_URLS)} ä¸ªé“¾æ¥ï¼Œå¼€å§‹è‡ªåŠ¨å¤„ç†...")
            target_urls = list(BATCH_URLS)
            BATCH_URLS.clear()
            should_prompt = True
        else:
            try:
                if should_prompt:
                    print("\nğŸ”— è¯·ç²˜è´´çŸ¥ä¹é“¾æ¥ (å¯åŒ…å«å…¶å®ƒæ–‡å­—): ", end="", flush=True)
                    should_prompt = False

                user_input = sys.stdin.readline().strip()
            except (EOFError, KeyboardInterrupt):
                print("\nğŸ‘‹ å†è§!")
                break

            if user_input.lower() == "q":
                print("ğŸ‘‹ å†è§!")
                break

            target_urls = extract_urls(user_input)

        if not target_urls:
            # åªæœ‰å½“ç”¨æˆ·è¾“å…¥ä¸ºç©ºæ—¶æ‰æç¤ºï¼Œé¿å…å› ä¸ºå¤åˆ¶äº†æ ‡é¢˜è¡Œå¯¼è‡´æŠ¥é”™åˆ·å±
            if not user_input:
                 should_prompt = True
            continue

        # â”€â”€ é€ä¸ªå¤„ç† â”€â”€
        print(f"ğŸ” æ£€æµ‹åˆ° {len(target_urls)} ä¸ªé“¾æ¥")

        for url in target_urls:
            print(f"\n{'='*60}")
            
            # æ£€æµ‹æ˜¯å¦ä¸ºé—®é¢˜é“¾æ¥ï¼Œå¦‚æœæ˜¯ï¼Œè¯¢é—®æŠ“å–èŒƒå›´
            scrape_config = {}
            if "/question/" in url and "/answer/" not in url:
                try:
                    print(f"âš™ï¸  æ£€æµ‹åˆ°é—®é¢˜é“¾æ¥: {url}")
                    print("   è¯·é€‰æ‹©æŠ“å–æ¨¡å¼:")
                    print("   [Enter] é»˜è®¤ (å‰ 20 ä¸ª)")
                    print("   [  s  ] æ™ºèƒ½æ¨¡å¼ (èµæ•°æ¯”ä¾‹åœæ­¢ï¼Œæœ€å¤š 10 æ¡)")
                    print("   [ 50  ] æŠ“å–å‰ 50 ä¸ª")
                    print("   [10-20] æŠ“å–ç¬¬ 10 åˆ° 20 ä¸ª")
                    print("ğŸ‘‰ è¯·è¾“å…¥: ", end="", flush=True)
                    opt_input = sys.stdin.readline().strip()
                    scrape_config = parse_question_options(opt_input)
                    if scrape_config.get("smart_stop"):
                        print(f"âœ… å·²è®¾å®š: æ™ºèƒ½æŠ“å–æ¨¡å¼")
                    else:
                        print(f"âœ… å·²è®¾å®š: Start={scrape_config['start']}, Limit={scrape_config['limit']}")
                except (KeyboardInterrupt, EOFError):
                    print("\nğŸ›‘ å–æ¶ˆæ“ä½œ")
                    continue

            print(f"ğŸ“¥ æ­£åœ¨æŠ“å–: {url}")
            try:
                await Pipeline(url, scrape_config=scrape_config).run()
            except Exception as e:
                err_msg = str(e)
                if "ERR_PROXY_CONNECTION_FAILED" in err_msg or "Connection refused" in err_msg:
                    print(f"\nâŒ ä»£ç†è¿æ¥å¤±è´¥: {e}")
                    print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥æœ¬åœ°ä»£ç†æ˜¯å¦å¼€å¯ï¼Œæˆ–åœ¨ scraper.py ä¸­å°† PROXY_SERVER è®¾ä¸º Noneã€‚")
                else:
                    print(f"âŒ å¤„ç†å¤±è´¥ [{url}]: {e}")
                print("ğŸ”„ è·³è¿‡å½“å‰é“¾æ¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...")

        print(f"\nâœ¨ æœ¬æ‰¹æ¬¡å¤„ç†å®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨ {DATA_DIR.resolve()}")
        should_prompt = True


if __name__ == "__main__":
    asyncio.run(main())