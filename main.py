import asyncio
import re
import sys
from datetime import datetime
from pathlib import Path

from typing import Optional
import functools
from concurrent.futures import ThreadPoolExecutor
import questionary
from rich.console import Console, Group
from rich.panel import Panel
from rich.table import Table
from rich.align import Align
from rich.text import Text
from rich.progress import track
from rich import print as rprint

from core.converter import ZhihuConverter
from core.scraper import ZhihuDownloader, PROXY_SERVER

# åˆå§‹åŒ– Rich Console
console = Console()
executor = ThreadPoolExecutor(max_workers=1)

# ==========================================
# æ‰¹é‡ä¸‹è½½åˆ—è¡¨ (ä¸æƒ³ç”¨å‘½ä»¤è¡Œè¾“å…¥æ—¶ï¼Œåœ¨è¿™é‡Œå¡«å…¥é“¾æ¥)
# ==========================================
BATCH_URLS = []

DATA_DIR = Path(__file__).parent / "data"

async def _async_input(prompt: str) -> str:
    """å°è£… rich çš„ console.input ä¸ºå¼‚æ­¥æ¨¡å¼ï¼Œæ¯” questionary.text ç¨³å®šã€‚"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, console.input, prompt)


# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def sanitize_filename(name: str) -> str:
    """æ¸…ç†æ–‡ä»¶åå¸¸ç”¨éæ³•å­—ç¬¦ã€‚"""
    name = re.sub(r'[/\\:*?"<>|\x00-\x1f]', "_", name)
    name = name.strip(" .")
    if len(name) > 50:
        name = name[:50].rstrip(" .")
    return name or "untitled"


def extract_urls(text: str) -> list[str]:
    """ä»æ–‡æœ¬ä¸­æå–çŸ¥ä¹é“¾æ¥ã€‚"""
    pattern = r"https?://(?:www\.|zhuanlan\.)?zhihu\.com/(?:p/\d+|question/\d+(?:/answer/\d+)?)"
    return list(dict.fromkeys(re.findall(pattern, text)))


def _print_banner():
    """æ‰“å°çœŸæ­£é…·ç‚«çš„ã€å®Œç¾å¯¹é½çš„ Bannerã€‚"""

    # 1. å‡†å¤‡ Banner å†…å®¹
    zh_text = Text("çŸ¥    ä¹    çˆ¬    è™«", style="bold cyan")
    
    # æ›´åŠ ç´§å‡‘ä¸”æ¸…æ™°çš„ ASCII å­—ä½“
    en_banner_raw = r"""
  ____  _   _ ___ _   _ _   _      ____   ____ ____      _    ____  _____ ____  
 |_  / | | | |_ _| | | | | | |    / ___| / ___|  _ \    / \  |  _ \| ____|  _ \ 
  / /  | |_| || || |_| | | | |    \___ \| |   | |_) |  / _ \ | |_) |  _| | |_) |
 / /_  |  _  || ||  _  | |_| |     ___) | |___|  _ <  / ___ \|  __/| |___|  _ < 
/____| |_| |_|___|_| |_|\___/     |____/ \____|_| \_\/_/   \_\_|   |_____|_| \_\
    """
    en_text = Text(en_banner_raw, style="bold dodger_blue1")

    # 2. å‡†å¤‡çŠ¶æ€è¡¨æ ¼
    proxy = "æœªæ£€æµ‹åˆ°"
    if PROXY_SERVER:
        proxy = PROXY_SERVER
        
    cookie_status = "[green]å·²é…ç½®[/green]" if (Path("cookies.json").exists()) else "[red]æœªé…ç½®[/red]"
    
    info_table = Table.grid(padding=(0, 2))
    info_table.add_column(style="bold magenta", justify="right")
    info_table.add_column()
    info_table.add_row("Version:", "2.1.0")
    info_table.add_row("Proxy:", proxy)
    info_table.add_row("Cookie:", cookie_status)
    info_table.add_row("Output:", str(DATA_DIR))

    # 3. ç»„åˆå¹¶å±…ä¸­æ‰“å°
    banner_group = Group(
        Align.center(zh_text),
        Align.center(en_text),
        Align.center(Panel(
            info_table, 
            title="[bold yellow]System Status[/bold yellow]", 
            border_style="bright_blue",
            expand=False,
            padding=(1, 4)
        ))
    )
    
    console.print(banner_group)
    console.print("\n")


async def parse_question_options(url: str) -> dict:
    """äº¤äº’å¼è§£æé—®é¢˜æŠ“å–é€‰é¡¹ã€‚"""
    
    # 1. æ£€æŸ¥ Cookie (å¤ç”¨ downloader çš„é€»è¾‘)
    downloader = ZhihuDownloader(url)
    if not await downloader.has_valid_cookies():
        console.print("[yellow]âš ï¸  æœªæ£€æµ‹åˆ°æœ‰æ•ˆç™»å½•ä¿¡æ¯ (z_c0)ï¼Œå¼ºåˆ¶ä½¿ç”¨æ¸¸å®¢æ¨¡å¼ (Top 3)[/yellow]")
        return {"start": 0, "limit": 3}

    # 2. äº¤äº’èœå• (å¼‚æ­¥)
    choice = await questionary.select(
        "è¯·é€‰æ‹©æŠ“å–æ¨¡å¼:",
        choices=[
            "1. æŒ‰æ•°é‡æŠ“å– (Top N)",
            "2. æŒ‰èŒƒå›´æŠ“å– (Start -> End)",
            "3. è¿”å›é»˜è®¤ (Top 3)"
        ]
    ).ask_async()
    
    if not choice: # Ctrl+C
        return {"start": 0, "limit": 3}
        
    if choice.startswith("1"):
        limit = await questionary.text(
            "è¯·è¾“å…¥æŠ“å–æ•°é‡:",
            default="20",
            validate=lambda text: text.isdigit() and int(text) > 0 or "è¯·è¾“å…¥æ­£æ•´æ•°"
        ).ask_async()
        return {"start": 0, "limit": int(limit) if limit else 3}
        
    elif choice.startswith("2"):
        console.print("[dim]æç¤º: æ”¯æŒè¾“å…¥ 'ç­”ä¸»åå­—' æˆ– 'å›ç­”é“¾æ¥/ID'[/dim]")
        start = await questionary.text("èµ·å§‹é”šç‚¹ (Start):").ask_async()
        end = await questionary.text("ç»“æŸé”šç‚¹ (End):").ask_async()
        
        s_anchor = _parse_anchor(start)
        e_anchor = _parse_anchor(end)
        
        if s_anchor and e_anchor:
            return {
                "start": 0, "limit": 3,
                "start_anchor": s_anchor,
                "end_anchor": e_anchor
            }
        else:
            console.print("[red]âŒ é”šç‚¹è§£æå¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤æ¨¡å¼[/red]")
            return {"start": 0, "limit": 3}
            
    return {"start": 0, "limit": 3}


def _parse_anchor(val: str) -> Optional[dict]:
    if not val: return None
    m = re.search(r"answer/(\d+)", val)
    if m: return {"type": "answer_id", "value": m.group(1)}
    return {"type": "author", "value": val}


# â”€â”€ æµæ°´çº¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Pipeline:
    def __init__(self, url: str, output_dir: Path = DATA_DIR, scrape_config: Optional[dict] = None):
        self.url = url
        self.output_dir = output_dir
        self.scrape_config = scrape_config or {}
        self.summary = [] # è®°å½•ç»“æœç”¨äºè¡¨æ ¼å±•ç¤º

    async def run(self) -> list:
        downloader = ZhihuDownloader(self.url)
        
        # ä½¿ç”¨ Status Spinner ä»£æ›¿åˆ·å±æ—¥å¿—
        with console.status(f"[bold green]æ­£åœ¨è¯·æ±‚é¡µé¢...[/bold green] {self.url}", spinner="dots"):
            data = await downloader.fetch_page(**self.scrape_config)

        if isinstance(data, list):
            console.print(f"ğŸ“¦ æŠ“å–åˆ° [bold cyan]{len(data)}[/bold cyan] ä¸ªå†…å®¹ï¼Œå¼€å§‹å¤„ç†...")
            # æ‰¹é‡å¤„ç†è¿›åº¦æ¡? è¿™é‡Œç®€å•èµ·è§è¿˜æ˜¯é€ä¸ªå¤„ç†ï¼Œä¸ºäº† Vibe æ•ˆæœï¼Œå¯ä»¥ç”¨ track
            
            for item in track(data, description="æ­£åœ¨è½¬æ¢æ–‡æ¡£..."):
                res = await self._process_one(item, downloader.page_type)
                self.summary.append(res)
        else:
            res = await self._process_one(data, downloader.page_type)
            self.summary.append(res)
            
        return self.summary

    async def _process_one(self, info: dict, page_type: str) -> dict:
        title = info["title"]
        author = info["author"]
        html = info["html"]
        
        # ç»“æœå¯¹è±¡
        result = {
            "title": title,
            "author": author,
            "status": "âœ… æˆåŠŸ",
            "path": ""
        }

        try:
            today = datetime.now().strftime("%Y-%m-%d")
            safe_title = sanitize_filename(title)
            safe_author = sanitize_filename(author)

            if page_type == "question":
                folder_name = sanitize_filename(f"[{today}] {title}")
                file_name = f"{safe_author}.md"
            else:
                date_str = info.get("date", today)
                folder_name = sanitize_filename(f"[{date_str}] {title} - {author}")
                file_name = "index.md"

            folder = self.output_dir / folder_name
            img_dir = folder / "images"
            folder.mkdir(parents=True, exist_ok=True)

            # ä¸‹è½½å›¾ç‰‡
            img_urls = ZhihuConverter.extract_image_urls(html)
            img_map = {}
            if img_urls:
                # è¿™é‡Œçš„ print ä¼šæ‰“æ–­ progress barï¼Œä½†åœ¨ single item æ—¶æ²¡å…³ç³»
                # ä¸ºäº†å®Œç¾ï¼Œè¿™é‡Œå…ˆé™é»˜ä¸‹è½½æˆ–ç®€å•è¾“å‡º
                img_map = await ZhihuDownloader.download_images(img_urls, img_dir)

            # HTML -> Markdown
            converter = ZhihuConverter(img_map=img_map)
            md = converter.convert(html)

            # ä¿å­˜
            header = (
                f"# {title}\n\n"
                f"> **ä½œè€…**: {author}  \n"
                f"> **æ¥æº**: [{self.url}]({self.url})  \n"
                f"> **æ—¥æœŸ**: {today}\n\n"
                f"---\n\n"
            )
            out_path = folder / file_name
            out_path.write_text(header + md, encoding="utf-8")
            
            # æ¸…ç†ç©ºç›®å½•
            if img_dir.exists() and not any(img_dir.iterdir()):
                img_dir.rmdir()
                
            # è®°å½•ç›¸å¯¹è·¯å¾„
            try:
                result["path"] = str(out_path.relative_to(Path.cwd()))
            except:
                result["path"] = str(out_path)
                
        except Exception as e:
            result["status"] = f"âŒ å¤±è´¥: {e}"
        
        return result


# â”€â”€ ä¸»å¾ªç¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main() -> None:
    _print_banner()
    
    while True:
        # è·å–è¾“å…¥
        if BATCH_URLS:
            console.print(f"[bold yellow]ğŸ“‹ æ£€æµ‹åˆ°æ‰¹é‡ä»»åŠ¡ ({len(BATCH_URLS)} ä¸ª)[/bold yellow]")
            urls = list(BATCH_URLS)
            BATCH_URLS.clear()
        else:
            # ä½¿ç”¨ rich åŸç”Ÿ input çš„å°è£…ç‰ˆï¼Œå½»åº•è§£å†³ ghost prompt å†²çªé—®é¢˜
            answer = await _async_input("ğŸ”— [bold cyan]è¾“å…¥çŸ¥ä¹é“¾æ¥ (æˆ– 'q' é€€å‡º): [/]")
            
            if not answer or answer.strip().lower() == 'q':
                console.print("[bold cyan]ğŸ‘‹ See you next time![/bold cyan]")
                break
            
            answer = answer.strip()
            urls = extract_urls(answer)
            
        if not urls:
            if answer and answer.lower() != 'q':
                # æ‰¹é‡ä»»åŠ¡æ— éœ€æŠ¥é”™ï¼Œæ­£å¸¸å¾ªç¯
                if not BATCH_URLS:
                    console.print("[red]âŒ æœªè¯†åˆ«åˆ°æœ‰æ•ˆé“¾æ¥ï¼Œè¯·é‡è¯•[/red]")
            continue
            
        # å¤„ç†é“¾æ¥
        console.rule(f"[bold]å¼€å§‹å¤„ç† {len(urls)} ä¸ªä»»åŠ¡[/bold]")
        
        all_results = []
        
        for url in urls:
            scrape_config = {}
            if "/question/" in url and "/answer/" not in url:
                console.print(f"\n[bold cyan]âš™ï¸  æ£€æµ‹åˆ°é—®é¢˜é“¾æ¥:[/bold cyan] {url}")
                scrape_config = await parse_question_options(url)
            
            try:
                pipeline = Pipeline(url, scrape_config=scrape_config)
                results = await pipeline.run()
                all_results.extend(results)
            except Exception as e:
                console.print(f"[bold red]âŒ ä¸¥é‡é”™è¯¯:[/bold red] {e}")
        
        # æ‰“å°æ±‡æ€»è¡¨æ ¼
        if all_results:
            table = Table(title="âœ… ä»»åŠ¡æ‰§è¡Œæ±‡æ€»", show_header=True, header_style="bold magenta")
            table.add_column("ä½œè€…/æ ‡é¢˜", style="dim")
            table.add_column("çŠ¶æ€", justify="center")
            table.add_column("ä¿å­˜è·¯å¾„", style="green")
            
            for res in all_results:
                table.add_row(
                    f"{res['author']}\n[dim]{res['title'][:20]}[/dim]",
                    res['status'],
                    res['path']
                )
            
            console.print(table)
            console.print("\n")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass